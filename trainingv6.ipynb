{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 훈련 진행 중... 에포크 6/10\n",
      "\n",
      "현재 훈련 지표:\n",
      "에포크 스텝    훈련 손실    검증 손실  의도 정확도  의도 F1 점수  개체인식 F1 점수    종합 점수\n",
      "  1 13 2.044167 1.133590     1.0       1.0    0.000000 0.500000\n",
      "  2 26 0.765911 0.413709     1.0       1.0    0.000000 0.500000\n",
      "  3 39 0.361759 0.229863     1.0       1.0    0.000000 0.500000\n",
      "  4 52 0.216557 0.141529     1.0       1.0    0.761905 0.880952\n",
      "  5 65 0.145732 0.103934     1.0       1.0    0.575000 0.787500\n",
      "  6 78 0.108834 0.077331     1.0       1.0    0.761905 0.880952\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import commentjson\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 장치: {device}\")\n",
    "\n",
    "# 한국어 특화 모델과 토크나이저 설정\n",
    "MODEL_NAME = \"klue/roberta-base\"  # 한국어 특화 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "with open('integrated_data.jsonc', 'r', encoding='utf-8') as f:\n",
    "        example_data = commentjson.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# NER 태그 설정\n",
    "ner_tags = [\"O\", \"B-BOOK\", \"I-BOOK\", \"B-AUTHOR\", \"I-AUTHOR\"]\n",
    "tag2id = {tag: idx for idx, tag in enumerate(ner_tags)}\n",
    "id2tag = {idx: tag for idx, tag in enumerate(ner_tags)}\n",
    "\n",
    "# Intent 라벨 설정\n",
    "intent_labels = [\"도서검색\", \"작가검색\"]\n",
    "intent2id = {intent: idx for idx, intent in enumerate(intent_labels)}\n",
    "id2intent = {idx: intent for idx, intent in enumerate(intent_labels)}\n",
    "\n",
    "def align_entity_with_tokens(text, entities, tokenizer):\n",
    "    \"\"\"\n",
    "    원본 텍스트와 엔티티 위치를 토큰화와 일치시키는 함수\n",
    "    \"\"\"\n",
    "    aligned_entities = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_text = text[entity[\"start\"]:entity[\"end\"]]\n",
    "        entity_tokens = tokenizer.tokenize(entity_text)\n",
    "        \n",
    "        # 전체 텍스트의 토큰화 결과에서 엔티티 시작 위치 찾기\n",
    "        prefix_text = text[:entity[\"start\"]]\n",
    "        prefix_tokens = tokenizer.tokenize(prefix_text)\n",
    "        \n",
    "        token_start = len(prefix_tokens)\n",
    "        token_end = token_start + len(entity_tokens) - 1\n",
    "        \n",
    "        aligned_entities.append({\n",
    "            \"type\": entity[\"type\"],\n",
    "            \"token_start\": token_start,\n",
    "            \"token_end\": token_end,\n",
    "            \"start\": entity[\"start\"],\n",
    "            \"end\": entity[\"end\"]\n",
    "        })\n",
    "    \n",
    "    return aligned_entities\n",
    "\n",
    "\n",
    "# 데이터셋 클래스 정의 (한국어 특화)\n",
    "class KoreanLibraryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, tag2id, intent2id):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2id = tag2id\n",
    "        self.intent2id = intent2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        intent = item[\"intent\"]\n",
    "        entities = item[\"entities\"]\n",
    "        \n",
    "        # 토큰화 전에 원본 텍스트와 엔티티 위치 정렬\n",
    "        aligned_entities = align_entity_with_tokens(text, entities, self.tokenizer)\n",
    "        \n",
    "        # 토큰화 (special_tokens 포함)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        # NER 레이블 초기화 (모두 O 태그)\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * self.tag2id[\"O\"]\n",
    "        \n",
    "        # CLS 토큰 위치 계산\n",
    "        cls_token_id = self.tokenizer.cls_token_id\n",
    "        sep_token_id = self.tokenizer.sep_token_id\n",
    "        \n",
    "        # CLS 토큰 위치 찾기 (일반적으로 0)\n",
    "        cls_position = 0\n",
    "        \n",
    "        # 각 엔티티에 대해 BIO 태그 적용 (special token 고려)\n",
    "        for entity in aligned_entities:\n",
    "            entity_type = entity[\"type\"]\n",
    "            # CLS 토큰 때문에 오프셋 +1\n",
    "            token_start = entity[\"token_start\"] + 1\n",
    "            token_end = entity[\"token_end\"] + 1\n",
    "            \n",
    "            # 시작 토큰에 B- 태그 적용\n",
    "            if token_start < self.max_len:\n",
    "                labels[token_start] = self.tag2id[f\"B-{entity_type}\"]\n",
    "            \n",
    "            # 나머지 토큰에 I- 태그 적용\n",
    "            for i in range(token_start + 1, token_end + 1):\n",
    "                if i < self.max_len:\n",
    "                    labels[i] = self.tag2id[f\"I-{entity_type}\"]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"intent_label\": torch.tensor(intent, dtype=torch.long),\n",
    "            \"ner_labels\": labels\n",
    "        }\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_data = example_data  # 실제로는 train/valid로 나눠야 함\n",
    "valid_data = example_data[:3]  # 예시용 간단한 검증 세트\n",
    "\n",
    "train_dataset = KoreanLibraryDataset(train_data, tokenizer, MAX_LENGTH, tag2id, intent2id)\n",
    "valid_dataset = KoreanLibraryDataset(valid_data, tokenizer, MAX_LENGTH, tag2id, intent2id)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 통합 모델 정의 (Intent 분류와 NER을 동시에 수행)\n",
    "class JointIntentNERModel(nn.Module):\n",
    "    def __init__(self, model_name, num_intents, num_ner_tags, dropout_prob=0.1):\n",
    "        super(JointIntentNERModel, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Intent 분류를 위한 레이어\n",
    "        self.intent_classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(self.roberta.config.hidden_size, num_intents)\n",
    "        )\n",
    "        \n",
    "        # NER을 위한 레이어\n",
    "        self.ner_classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(self.roberta.config.hidden_size, num_ner_tags)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = sequence_output[:, 0, :]  # [CLS] 토큰 임베딩 사용\n",
    "        \n",
    "        # Intent 분류\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        \n",
    "        # NER 태그 분류\n",
    "        ner_logits = self.ner_classifier(sequence_output)\n",
    "        \n",
    "        return intent_logits, ner_logits\n",
    "\n",
    "# 모델 초기화\n",
    "model = JointIntentNERModel(\n",
    "    model_name=MODEL_NAME, \n",
    "    num_intents=len(intent_labels), \n",
    "    num_ner_tags=len(ner_tags)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 및 스케줄러 설정\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 손실 함수\n",
    "intent_criterion = nn.CrossEntropyLoss()\n",
    "ner_criterion = nn.CrossEntropyLoss(ignore_index=-100)  # padding된 부분은 손실 계산에서 제외\n",
    "\n",
    "\n",
    "\n",
    "# 모델 저장 함수\n",
    "def save_model(model, tokenizer, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "    \n",
    "    # 토크나이저 저장\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # 모델 설정 저장\n",
    "    config = {\n",
    "        \"intent_labels\": intent_labels,\n",
    "        \"ner_tags\": ner_tags,\n",
    "        \"max_length\": MAX_LENGTH\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def train_and_evaluate(model, train_dataloader, valid_dataloader, optimizer, scheduler, \n",
    "                      device, num_epochs, output_dir):\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # 전체 훈련 과정을 위한 단일 진행 표시줄\n",
    "    total_steps = num_epochs * (len(train_dataloader) + len(valid_dataloader) + 2)  # +2는 에포크별 요약 단계\n",
    "    main_progress_bar = tqdm(total=total_steps, desc=\"훈련 진행률\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 에포크 정보 업데이트 (tqdm에서 표시)\n",
    "        main_progress_bar.set_description(f\"에포크 {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # 훈련 단계\n",
    "        model.train()\n",
    "        total_intent_loss = 0\n",
    "        total_ner_loss = 0\n",
    "        intent_preds, intent_labels = [], []\n",
    "        ner_preds, ner_true = [], []\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            intent_label = batch[\"intent_label\"].to(device)\n",
    "            ner_labels = batch[\"ner_labels\"].to(device)\n",
    "            \n",
    "            # 모델 예측\n",
    "            intent_logits, ner_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Intent 손실 계산\n",
    "            intent_loss = intent_criterion(intent_logits, intent_label)\n",
    "            \n",
    "            # NER 손실 계산 (모든 토큰에 대해)\n",
    "            ner_loss = ner_criterion(ner_logits.view(-1, len(ner_tags)), ner_labels.view(-1))\n",
    "            \n",
    "            # 전체 손실 (NER 손실과 Intent 손실의 가중합)\n",
    "            loss = intent_loss + ner_loss\n",
    "            \n",
    "            # 역전파 및 옵티마이저 스텝\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 그래디언트 클리핑\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 손실 누적\n",
    "            total_intent_loss += intent_loss.item()\n",
    "            total_ner_loss += ner_loss.item()\n",
    "            \n",
    "            # Intent 예측값 및 실제값 저장\n",
    "            intent_preds.extend(torch.argmax(intent_logits, dim=1).cpu().numpy())\n",
    "            intent_labels.extend(intent_label.cpu().numpy())\n",
    "            \n",
    "            # NER 예측값 및 실제값 저장\n",
    "            ner_pred = torch.argmax(ner_logits, dim=2).cpu().numpy()\n",
    "            ner_true_labels = ner_labels.cpu().numpy()\n",
    "            \n",
    "            # attention_mask를 사용하여 패딩된 부분을 제외하고 예측 및 실제 NER 태그 저장\n",
    "            for i, mask in enumerate(attention_mask.cpu().numpy()):\n",
    "                pred_tags = [id2tag[p] for j, p in enumerate(ner_pred[i]) if mask[j] == 1]\n",
    "                true_tags = [id2tag[t] if t != -100 else \"O\" for j, t in enumerate(ner_true_labels[i]) if mask[j] == 1]\n",
    "                ner_preds.append(pred_tags)\n",
    "                ner_true.append(true_tags)\n",
    "            \n",
    "            # 진행 상황 업데이트\n",
    "            train_intent_loss = total_intent_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "            train_ner_loss = total_ner_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "            \n",
    "            main_progress_bar.set_postfix({\n",
    "                '단계': '훈련',\n",
    "                'intent_loss': f\"{train_intent_loss:.4f}\",\n",
    "                'ner_loss': f\"{train_ner_loss:.4f}\"\n",
    "            })\n",
    "            main_progress_bar.update(1)  # 프로그레스 바 1단계 진행\n",
    "        \n",
    "        # 훈련 평가 지표 계산\n",
    "        train_intent_accuracy = accuracy_score(intent_labels, intent_preds)\n",
    "        train_intent_f1 = f1_score(intent_labels, intent_preds, average='weighted')\n",
    "        \n",
    "        # NER 평가 지표 계산 (seqeval 사용)\n",
    "        train_ner_report = seqeval_report(ner_true, ner_preds, output_dict=True, zero_division=0)\n",
    "        train_ner_f1 = train_ner_report['weighted avg']['f1-score']\n",
    "        \n",
    "        # 평가 단계 \n",
    "        model.eval()\n",
    "        total_intent_loss = 0\n",
    "        total_ner_loss = 0\n",
    "        intent_preds, intent_labels_list = [], []\n",
    "        ner_preds, ner_true = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                intent_label = batch[\"intent_label\"].to(device)\n",
    "                ner_labels = batch[\"ner_labels\"].to(device)\n",
    "                \n",
    "                # 모델 예측\n",
    "                intent_logits, ner_logits = model(input_ids, attention_mask)\n",
    "                \n",
    "                # Intent 손실 계산\n",
    "                intent_loss = intent_criterion(intent_logits, intent_label)\n",
    "                \n",
    "                # NER 손실 계산\n",
    "                ner_loss = ner_criterion(ner_logits.view(-1, len(ner_tags)), ner_labels.view(-1))\n",
    "                \n",
    "                # 손실 누적\n",
    "                total_intent_loss += intent_loss.item()\n",
    "                total_ner_loss += ner_loss.item()\n",
    "                \n",
    "                # Intent 예측값 및 실제값 저장\n",
    "                intent_preds.extend(torch.argmax(intent_logits, dim=1).cpu().numpy())\n",
    "                intent_labels_list.extend(intent_label.cpu().numpy())\n",
    "                \n",
    "                # NER 예측값 및 실제값 저장\n",
    "                ner_pred = torch.argmax(ner_logits, dim=2).cpu().numpy()\n",
    "                ner_true_labels = ner_labels.cpu().numpy()\n",
    "                \n",
    "                # attention_mask를 사용하여 패딩된 부분을 제외하고 예측 및 실제 NER 태그 저장\n",
    "                for i, mask in enumerate(attention_mask.cpu().numpy()):\n",
    "                    pred_tags = [id2tag[p] for j, p in enumerate(ner_pred[i]) if mask[j] == 1]\n",
    "                    true_tags = [id2tag[t] if t != -100 else \"O\" for j, t in enumerate(ner_true_labels[i]) if mask[j] == 1]\n",
    "                    ner_preds.append(pred_tags)\n",
    "                    ner_true.append(true_tags)\n",
    "                \n",
    "                # 진행 상황 업데이트 \n",
    "                val_intent_loss = total_intent_loss / len(valid_dataloader) if len(valid_dataloader) > 0 else 0\n",
    "                val_ner_loss = total_ner_loss / len(valid_dataloader) if len(valid_dataloader) > 0 else 0\n",
    "                \n",
    "                main_progress_bar.set_postfix({\n",
    "                    '단계': '평가',\n",
    "                    'intent_loss': f\"{val_intent_loss:.4f}\",\n",
    "                    'ner_loss': f\"{val_ner_loss:.4f}\"\n",
    "                })\n",
    "                main_progress_bar.update(1)  # 프로그레스 바 1단계 진행\n",
    "        \n",
    "        # Intent 평가 지표 계산\n",
    "        val_intent_accuracy = accuracy_score(intent_labels_list, intent_preds)\n",
    "        val_intent_f1 = f1_score(intent_labels_list, intent_preds, average='weighted')\n",
    "        \n",
    "        # 데이터셋에 실제로 존재하는 클래스 확인 (동적으로 처리)\n",
    "        unique_labels = sorted(set(intent_labels_list))\n",
    "        used_target_names = [id2intent[i] for i in unique_labels]\n",
    "        \n",
    "        # 실제 존재하는 클래스에 대해서만 classification_report 생성\n",
    "        intent_report = classification_report(\n",
    "            intent_labels_list, \n",
    "            intent_preds,\n",
    "            target_names=used_target_names, \n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        # NER 평가 지표 계산\n",
    "        ner_report = seqeval_report(ner_true, ner_preds, output_dict=True, zero_division=0)\n",
    "        val_ner_f1 = ner_report['weighted avg']['f1-score']\n",
    "        \n",
    "        # 에포크 결과 요약 \n",
    "        avg_f1 = (val_intent_f1 + val_ner_f1) / 2\n",
    "        \n",
    "        # 진행바에 에포크 결과 표시\n",
    "        main_progress_bar.set_postfix({\n",
    "            'Ep': f\"{epoch+1}/{num_epochs}\",\n",
    "            'intent_acc': f\"{val_intent_accuracy:.4f}\",\n",
    "            'avg_f1': f\"{avg_f1:.4f}\"\n",
    "        })\n",
    "        main_progress_bar.update(1)  # 에포크 요약 단계 업데이트\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            save_model(model, tokenizer, output_dir)\n",
    "            main_progress_bar.set_postfix({\n",
    "                'Ep': f\"{epoch+1}/{num_epochs}\",\n",
    "                'best_avg_f1': f\"{avg_f1:.4f}\",\n",
    "                'saved': 'Y'\n",
    "            })\n",
    "        \n",
    "        main_progress_bar.update(1)  # 모델 저장 단계 업데이트\n",
    "    \n",
    "    main_progress_bar.close()\n",
    "    return best_f1\n",
    "\n",
    "# 훈련 실행\n",
    "best_f1 = 0\n",
    "output_dir = \"./korean_library_chatbot_model\"\n",
    "logging_steps = 10  # 10배치마다 진행 상황 업데이트\n",
    "\n",
    "# 훈련 시작\n",
    "print(\"\\n훈련 시작...\")\n",
    "best_f1 = train_and_evaluate(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=EPOCHS,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "print(f\"\\n훈련 완료! 최종 모델이 {output_dir}에 저장되었습니다. (최고 평균 F1: {best_f1:.4f})\")\n",
    "\n",
    "# 실제 텍스트에서 엔티티를 추출하는 함수 개선\n",
    "def extract_entities_from_tokens(text, token_predictions, offset_mapping, id2tag):\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, (pred, (start, end)) in enumerate(zip(token_predictions, offset_mapping)):\n",
    "        if start == end:  # 특수 토큰 건너뛰기\n",
    "            continue\n",
    "            \n",
    "        tag = id2tag[pred]\n",
    "        \n",
    "        if tag.startswith(\"B-\"):  # 엔티티 시작\n",
    "            # 이전 엔티티가 있으면 저장\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            entity_type = tag[2:]  # \"B-\" 제거\n",
    "            current_entity = {\n",
    "                \"type\": entity_type,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"text\": text[start:end]\n",
    "            }\n",
    "        \n",
    "        elif tag.startswith(\"I-\") and current_entity is not None:\n",
    "            # 이전 엔티티와 같은 타입인 경우만 확장\n",
    "            if current_entity[\"type\"] == tag[2:]:\n",
    "                current_entity[\"end\"] = end\n",
    "                current_entity[\"text\"] = text[current_entity[\"start\"]:end]\n",
    "        \n",
    "        elif tag == \"O\":  # 엔티티 종료\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # 마지막 엔티티 처리\n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    # 중복 엔티티 제거 (더 긴 엔티티 우선)\n",
    "    filtered_entities = []\n",
    "    for entity in sorted(entities, key=lambda e: len(e[\"text\"]), reverse=True):\n",
    "        # 이미 포함된 엔티티인지 확인\n",
    "        is_contained = False\n",
    "        for filtered in filtered_entities:\n",
    "            if (entity[\"start\"] >= filtered[\"start\"] and \n",
    "                entity[\"end\"] <= filtered[\"end\"] and\n",
    "                entity[\"type\"] == filtered[\"type\"]):\n",
    "                is_contained = True\n",
    "                break\n",
    "        \n",
    "        if not is_contained:\n",
    "            filtered_entities.append(entity)\n",
    "    \n",
    "    return filtered_entities\n",
    "\n",
    "# 모델 추론 함수 개선\n",
    "def predict_intent_and_entities(text, model, tokenizer, id2intent, id2tag):\n",
    "    model.eval()\n",
    "    \n",
    "    # 토큰화 (오프셋 매핑 포함)\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    offset_mapping = encoding[\"offset_mapping\"].squeeze().cpu().numpy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        intent_logits, ner_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Intent 예측\n",
    "        intent_pred = torch.argmax(intent_logits, dim=1).cpu().numpy()[0]\n",
    "        intent_name = id2intent[intent_pred]\n",
    "        \n",
    "        # NER 예측\n",
    "        ner_pred = torch.argmax(ner_logits, dim=2).cpu().numpy()[0]\n",
    "        \n",
    "        # 토큰화된 결과 확인 (디버깅용)\n",
    "        print(\"\\n토큰화 결과:\")\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "        for i, (token, pred) in enumerate(zip(tokens, ner_pred)):\n",
    "            if attention_mask[0][i] == 1:  # 패딩이 아닌 토큰만\n",
    "                print(f\"  {token}: {id2tag[pred]}\")\n",
    "        \n",
    "        # 실제 텍스트 범위에 맞게 엔티티 추출\n",
    "        entities = extract_entities_from_tokens(text, ner_pred, offset_mapping, id2tag)\n",
    "    \n",
    "    return {\n",
    "        \"intent\": intent_name,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "def postprocess_prediction(text, prediction, common_authors_set=None, common_books_set=None):\n",
    "    current_intent = prediction[\"intent\"]\n",
    "    original_model_intent = prediction[\"intent\"] \n",
    "    entities = prediction[\"entities\"]\n",
    "    \n",
    "    has_author_entity = any(e[\"type\"] == \"AUTHOR\" for e in entities)\n",
    "    has_book_entity = any(e[\"type\"] == \"BOOK\" for e in entities)\n",
    "\n",
    "    # 시나리오 1: 작가 엔티티만 있고, 모델이 '도서검색'으로 예측한 경우 -> '작가검색'으로 보정\n",
    "    if original_model_intent == \"도서검색\" and has_author_entity and not has_book_entity:\n",
    "        current_intent = \"작가검색\" \n",
    "\n",
    "    # 시나리오 2: 책 엔티티만 있고, 모델이 '작가검색'으로 예측한 경우 -> '도서검색'으로 보정\n",
    "    elif original_model_intent == \"작가검색\" and has_book_entity and not has_author_entity:\n",
    "        current_intent = \"도서검색\"\n",
    "        \n",
    "    if original_model_intent != current_intent:\n",
    "        print(f\"INFO: Intent corrected by postprocessing. Original: '{original_model_intent}', Corrected: '{current_intent}'. Text: '{text}'\")\n",
    "\n",
    "    return {\n",
    "        \"intent\": current_intent,\n",
    "        \"entities\": entities,\n",
    "        \"original_intent\": original_model_intent,\n",
    "        \"text\": text  # 함수의 인자로 받은 text를 사용\n",
    "    }\n",
    "\n",
    "# 개선된 추론 실행\n",
    "print(\"\\n개선된 추론 예시:\")\n",
    "sample_texts = [\n",
    "    \"스즈미야 하루히의 우울 도서관에에 있나요?\",\n",
    "    \"전생했더니 슬라임이였던 건에 대하여 책 있어?\",\n",
    "    \"해리포터와 불의 잔 어디 있어?\",\n",
    "    \"박완서 작가에 대해 알려줘\"\n",
    "]\n",
    "\n",
    "for sample_text in sample_texts:\n",
    "    print(f\"\\n입력 텍스트: {sample_text}\")\n",
    "    \n",
    "    # 원본 예측\n",
    "    raw_prediction = predict_intent_and_entities(\n",
    "        sample_text, model, tokenizer, id2intent, id2tag\n",
    "    )\n",
    "    \n",
    "    # 후처리로 예측 개선\n",
    "    prediction = postprocess_prediction(sample_text, raw_prediction)\n",
    "    \n",
    "    print(f\"예측 결과:\")\n",
    "    print(f\"  의도: {prediction['intent']}\")\n",
    "    print(f\"  개체:\")\n",
    "    if not prediction['entities']:\n",
    "        print(\"    - 개체 없음\")\n",
    "    else:\n",
    "        for entity in prediction['entities']:\n",
    "            print(f\"    - {entity['type']}: '{entity['text']}' (위치: {entity['start']}-{entity['end']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
