{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ í›ˆë ¨ ì§„í–‰ ì¤‘... ì—í¬í¬ 6/10\n",
      "\n",
      "í˜„ì¬ í›ˆë ¨ ì§€í‘œ:\n",
      "ì—í¬í¬ ìŠ¤í…    í›ˆë ¨ ì†ì‹¤    ê²€ì¦ ì†ì‹¤  ì˜ë„ ì •í™•ë„  ì˜ë„ F1 ì ìˆ˜  ê°œì²´ì¸ì‹ F1 ì ìˆ˜    ì¢…í•© ì ìˆ˜\n",
      "  1 13 2.044167 1.133590     1.0       1.0    0.000000 0.500000\n",
      "  2 26 0.765911 0.413709     1.0       1.0    0.000000 0.500000\n",
      "  3 39 0.361759 0.229863     1.0       1.0    0.000000 0.500000\n",
      "  4 52 0.216557 0.141529     1.0       1.0    0.761905 0.880952\n",
      "  5 65 0.145732 0.103934     1.0       1.0    0.575000 0.787500\n",
      "  6 78 0.108834 0.077331     1.0       1.0    0.761905 0.880952\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import commentjson\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ì¥ì¹˜ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "\n",
    "# í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "MODEL_NAME = \"klue/roberta-base\"  # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "with open('integrated_data.jsonc', 'r', encoding='utf-8') as f:\n",
    "        example_data = commentjson.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# NER íƒœê·¸ ì„¤ì •\n",
    "ner_tags = [\"O\", \"B-BOOK\", \"I-BOOK\", \"B-AUTHOR\", \"I-AUTHOR\"]\n",
    "tag2id = {tag: idx for idx, tag in enumerate(ner_tags)}\n",
    "id2tag = {idx: tag for idx, tag in enumerate(ner_tags)}\n",
    "\n",
    "# Intent ë¼ë²¨ ì„¤ì •\n",
    "intent_labels = [\"ë„ì„œê²€ìƒ‰\", \"ì‘ê°€ê²€ìƒ‰\"]\n",
    "intent2id = {intent: idx for idx, intent in enumerate(intent_labels)}\n",
    "id2intent = {idx: intent for idx, intent in enumerate(intent_labels)}\n",
    "\n",
    "def align_entity_with_tokens(text, entities, tokenizer):\n",
    "    \"\"\"\n",
    "    ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ì—”í‹°í‹° ìœ„ì¹˜ë¥¼ í† í°í™”ì™€ ì¼ì¹˜ì‹œí‚¤ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    aligned_entities = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_text = text[entity[\"start\"]:entity[\"end\"]]\n",
    "        entity_tokens = tokenizer.tokenize(entity_text)\n",
    "        \n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ì˜ í† í°í™” ê²°ê³¼ì—ì„œ ì—”í‹°í‹° ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°\n",
    "        prefix_text = text[:entity[\"start\"]]\n",
    "        prefix_tokens = tokenizer.tokenize(prefix_text)\n",
    "        \n",
    "        token_start = len(prefix_tokens)\n",
    "        token_end = token_start + len(entity_tokens) - 1\n",
    "        \n",
    "        aligned_entities.append({\n",
    "            \"type\": entity[\"type\"],\n",
    "            \"token_start\": token_start,\n",
    "            \"token_end\": token_end,\n",
    "            \"start\": entity[\"start\"],\n",
    "            \"end\": entity[\"end\"]\n",
    "        })\n",
    "    \n",
    "    return aligned_entities\n",
    "\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (í•œêµ­ì–´ íŠ¹í™”)\n",
    "class KoreanLibraryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, tag2id, intent2id):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2id = tag2id\n",
    "        self.intent2id = intent2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        intent = item[\"intent\"]\n",
    "        entities = item[\"entities\"]\n",
    "        \n",
    "        # í† í°í™” ì „ì— ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ì—”í‹°í‹° ìœ„ì¹˜ ì •ë ¬\n",
    "        aligned_entities = align_entity_with_tokens(text, entities, self.tokenizer)\n",
    "        \n",
    "        # í† í°í™” (special_tokens í¬í•¨)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        # NER ë ˆì´ë¸” ì´ˆê¸°í™” (ëª¨ë‘ O íƒœê·¸)\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * self.tag2id[\"O\"]\n",
    "        \n",
    "        # CLS í† í° ìœ„ì¹˜ ê³„ì‚°\n",
    "        cls_token_id = self.tokenizer.cls_token_id\n",
    "        sep_token_id = self.tokenizer.sep_token_id\n",
    "        \n",
    "        # CLS í† í° ìœ„ì¹˜ ì°¾ê¸° (ì¼ë°˜ì ìœ¼ë¡œ 0)\n",
    "        cls_position = 0\n",
    "        \n",
    "        # ê° ì—”í‹°í‹°ì— ëŒ€í•´ BIO íƒœê·¸ ì ìš© (special token ê³ ë ¤)\n",
    "        for entity in aligned_entities:\n",
    "            entity_type = entity[\"type\"]\n",
    "            # CLS í† í° ë•Œë¬¸ì— ì˜¤í”„ì…‹ +1\n",
    "            token_start = entity[\"token_start\"] + 1\n",
    "            token_end = entity[\"token_end\"] + 1\n",
    "            \n",
    "            # ì‹œì‘ í† í°ì— B- íƒœê·¸ ì ìš©\n",
    "            if token_start < self.max_len:\n",
    "                labels[token_start] = self.tag2id[f\"B-{entity_type}\"]\n",
    "            \n",
    "            # ë‚˜ë¨¸ì§€ í† í°ì— I- íƒœê·¸ ì ìš©\n",
    "            for i in range(token_start + 1, token_end + 1):\n",
    "                if i < self.max_len:\n",
    "                    labels[i] = self.tag2id[f\"I-{entity_type}\"]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"intent_label\": torch.tensor(intent, dtype=torch.long),\n",
    "            \"ner_labels\": labels\n",
    "        }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_data = example_data  # ì‹¤ì œë¡œëŠ” train/validë¡œ ë‚˜ëˆ ì•¼ í•¨\n",
    "valid_data = example_data[:3]  # ì˜ˆì‹œìš© ê°„ë‹¨í•œ ê²€ì¦ ì„¸íŠ¸\n",
    "\n",
    "train_dataset = KoreanLibraryDataset(train_data, tokenizer, MAX_LENGTH, tag2id, intent2id)\n",
    "valid_dataset = KoreanLibraryDataset(valid_data, tokenizer, MAX_LENGTH, tag2id, intent2id)\n",
    "\n",
    "# ë°ì´í„°ë¡œë” ìƒì„±\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# í†µí•© ëª¨ë¸ ì •ì˜ (Intent ë¶„ë¥˜ì™€ NERì„ ë™ì‹œì— ìˆ˜í–‰)\n",
    "class JointIntentNERModel(nn.Module):\n",
    "    def __init__(self, model_name, num_intents, num_ner_tags, dropout_prob=0.1):\n",
    "        super(JointIntentNERModel, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Intent ë¶„ë¥˜ë¥¼ ìœ„í•œ ë ˆì´ì–´\n",
    "        self.intent_classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(self.roberta.config.hidden_size, num_intents)\n",
    "        )\n",
    "        \n",
    "        # NERì„ ìœ„í•œ ë ˆì´ì–´\n",
    "        self.ner_classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(self.roberta.config.hidden_size, num_ner_tags)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = sequence_output[:, 0, :]  # [CLS] í† í° ì„ë² ë”© ì‚¬ìš©\n",
    "        \n",
    "        # Intent ë¶„ë¥˜\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        \n",
    "        # NER íƒœê·¸ ë¶„ë¥˜\n",
    "        ner_logits = self.ner_classifier(sequence_output)\n",
    "        \n",
    "        return intent_logits, ner_logits\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = JointIntentNERModel(\n",
    "    model_name=MODEL_NAME, \n",
    "    num_intents=len(intent_labels), \n",
    "    num_ner_tags=len(ner_tags)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜\n",
    "intent_criterion = nn.CrossEntropyLoss()\n",
    "ner_criterion = nn.CrossEntropyLoss(ignore_index=-100)  # paddingëœ ë¶€ë¶„ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸\n",
    "\n",
    "\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ í•¨ìˆ˜\n",
    "def save_model(model, tokenizer, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # ëª¨ë¸ ì„¤ì • ì €ì¥\n",
    "    config = {\n",
    "        \"intent_labels\": intent_labels,\n",
    "        \"ner_tags\": ner_tags,\n",
    "        \"max_length\": MAX_LENGTH\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def train_and_evaluate(model, train_dataloader, valid_dataloader, optimizer, scheduler, \n",
    "                      device, num_epochs, output_dir):\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # ì „ì²´ í›ˆë ¨ ê³¼ì •ì„ ìœ„í•œ ë‹¨ì¼ ì§„í–‰ í‘œì‹œì¤„\n",
    "    total_steps = num_epochs * (len(train_dataloader) + len(valid_dataloader) + 2)  # +2ëŠ” ì—í¬í¬ë³„ ìš”ì•½ ë‹¨ê³„\n",
    "    main_progress_bar = tqdm(total=total_steps, desc=\"í›ˆë ¨ ì§„í–‰ë¥ \")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ì—í¬í¬ ì •ë³´ ì—…ë°ì´íŠ¸ (tqdmì—ì„œ í‘œì‹œ)\n",
    "        main_progress_bar.set_description(f\"ì—í¬í¬ {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # í›ˆë ¨ ë‹¨ê³„\n",
    "        model.train()\n",
    "        total_intent_loss = 0\n",
    "        total_ner_loss = 0\n",
    "        intent_preds, intent_labels = [], []\n",
    "        ner_preds, ner_true = [], []\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            intent_label = batch[\"intent_label\"].to(device)\n",
    "            ner_labels = batch[\"ner_labels\"].to(device)\n",
    "            \n",
    "            # ëª¨ë¸ ì˜ˆì¸¡\n",
    "            intent_logits, ner_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Intent ì†ì‹¤ ê³„ì‚°\n",
    "            intent_loss = intent_criterion(intent_logits, intent_label)\n",
    "            \n",
    "            # NER ì†ì‹¤ ê³„ì‚° (ëª¨ë“  í† í°ì— ëŒ€í•´)\n",
    "            ner_loss = ner_criterion(ner_logits.view(-1, len(ner_tags)), ner_labels.view(-1))\n",
    "            \n",
    "            # ì „ì²´ ì†ì‹¤ (NER ì†ì‹¤ê³¼ Intent ì†ì‹¤ì˜ ê°€ì¤‘í•©)\n",
    "            loss = intent_loss + ner_loss\n",
    "            \n",
    "            # ì—­ì „íŒŒ ë° ì˜µí‹°ë§ˆì´ì € ìŠ¤í…\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # ì†ì‹¤ ëˆ„ì \n",
    "            total_intent_loss += intent_loss.item()\n",
    "            total_ner_loss += ner_loss.item()\n",
    "            \n",
    "            # Intent ì˜ˆì¸¡ê°’ ë° ì‹¤ì œê°’ ì €ì¥\n",
    "            intent_preds.extend(torch.argmax(intent_logits, dim=1).cpu().numpy())\n",
    "            intent_labels.extend(intent_label.cpu().numpy())\n",
    "            \n",
    "            # NER ì˜ˆì¸¡ê°’ ë° ì‹¤ì œê°’ ì €ì¥\n",
    "            ner_pred = torch.argmax(ner_logits, dim=2).cpu().numpy()\n",
    "            ner_true_labels = ner_labels.cpu().numpy()\n",
    "            \n",
    "            # attention_maskë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ë”©ëœ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ì˜ˆì¸¡ ë° ì‹¤ì œ NER íƒœê·¸ ì €ì¥\n",
    "            for i, mask in enumerate(attention_mask.cpu().numpy()):\n",
    "                pred_tags = [id2tag[p] for j, p in enumerate(ner_pred[i]) if mask[j] == 1]\n",
    "                true_tags = [id2tag[t] if t != -100 else \"O\" for j, t in enumerate(ner_true_labels[i]) if mask[j] == 1]\n",
    "                ner_preds.append(pred_tags)\n",
    "                ner_true.append(true_tags)\n",
    "            \n",
    "            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸\n",
    "            train_intent_loss = total_intent_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "            train_ner_loss = total_ner_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "            \n",
    "            main_progress_bar.set_postfix({\n",
    "                'ë‹¨ê³„': 'í›ˆë ¨',\n",
    "                'intent_loss': f\"{train_intent_loss:.4f}\",\n",
    "                'ner_loss': f\"{train_ner_loss:.4f}\"\n",
    "            })\n",
    "            main_progress_bar.update(1)  # í”„ë¡œê·¸ë ˆìŠ¤ ë°” 1ë‹¨ê³„ ì§„í–‰\n",
    "        \n",
    "        # í›ˆë ¨ í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "        train_intent_accuracy = accuracy_score(intent_labels, intent_preds)\n",
    "        train_intent_f1 = f1_score(intent_labels, intent_preds, average='weighted')\n",
    "        \n",
    "        # NER í‰ê°€ ì§€í‘œ ê³„ì‚° (seqeval ì‚¬ìš©)\n",
    "        train_ner_report = seqeval_report(ner_true, ner_preds, output_dict=True, zero_division=0)\n",
    "        train_ner_f1 = train_ner_report['weighted avg']['f1-score']\n",
    "        \n",
    "        # í‰ê°€ ë‹¨ê³„ \n",
    "        model.eval()\n",
    "        total_intent_loss = 0\n",
    "        total_ner_loss = 0\n",
    "        intent_preds, intent_labels_list = [], []\n",
    "        ner_preds, ner_true = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                intent_label = batch[\"intent_label\"].to(device)\n",
    "                ner_labels = batch[\"ner_labels\"].to(device)\n",
    "                \n",
    "                # ëª¨ë¸ ì˜ˆì¸¡\n",
    "                intent_logits, ner_logits = model(input_ids, attention_mask)\n",
    "                \n",
    "                # Intent ì†ì‹¤ ê³„ì‚°\n",
    "                intent_loss = intent_criterion(intent_logits, intent_label)\n",
    "                \n",
    "                # NER ì†ì‹¤ ê³„ì‚°\n",
    "                ner_loss = ner_criterion(ner_logits.view(-1, len(ner_tags)), ner_labels.view(-1))\n",
    "                \n",
    "                # ì†ì‹¤ ëˆ„ì \n",
    "                total_intent_loss += intent_loss.item()\n",
    "                total_ner_loss += ner_loss.item()\n",
    "                \n",
    "                # Intent ì˜ˆì¸¡ê°’ ë° ì‹¤ì œê°’ ì €ì¥\n",
    "                intent_preds.extend(torch.argmax(intent_logits, dim=1).cpu().numpy())\n",
    "                intent_labels_list.extend(intent_label.cpu().numpy())\n",
    "                \n",
    "                # NER ì˜ˆì¸¡ê°’ ë° ì‹¤ì œê°’ ì €ì¥\n",
    "                ner_pred = torch.argmax(ner_logits, dim=2).cpu().numpy()\n",
    "                ner_true_labels = ner_labels.cpu().numpy()\n",
    "                \n",
    "                # attention_maskë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ë”©ëœ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ì˜ˆì¸¡ ë° ì‹¤ì œ NER íƒœê·¸ ì €ì¥\n",
    "                for i, mask in enumerate(attention_mask.cpu().numpy()):\n",
    "                    pred_tags = [id2tag[p] for j, p in enumerate(ner_pred[i]) if mask[j] == 1]\n",
    "                    true_tags = [id2tag[t] if t != -100 else \"O\" for j, t in enumerate(ner_true_labels[i]) if mask[j] == 1]\n",
    "                    ner_preds.append(pred_tags)\n",
    "                    ner_true.append(true_tags)\n",
    "                \n",
    "                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ \n",
    "                val_intent_loss = total_intent_loss / len(valid_dataloader) if len(valid_dataloader) > 0 else 0\n",
    "                val_ner_loss = total_ner_loss / len(valid_dataloader) if len(valid_dataloader) > 0 else 0\n",
    "                \n",
    "                main_progress_bar.set_postfix({\n",
    "                    'ë‹¨ê³„': 'í‰ê°€',\n",
    "                    'intent_loss': f\"{val_intent_loss:.4f}\",\n",
    "                    'ner_loss': f\"{val_ner_loss:.4f}\"\n",
    "                })\n",
    "                main_progress_bar.update(1)  # í”„ë¡œê·¸ë ˆìŠ¤ ë°” 1ë‹¨ê³„ ì§„í–‰\n",
    "        \n",
    "        # Intent í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "        val_intent_accuracy = accuracy_score(intent_labels_list, intent_preds)\n",
    "        val_intent_f1 = f1_score(intent_labels_list, intent_preds, average='weighted')\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ í™•ì¸ (ë™ì ìœ¼ë¡œ ì²˜ë¦¬)\n",
    "        unique_labels = sorted(set(intent_labels_list))\n",
    "        used_target_names = [id2intent[i] for i in unique_labels]\n",
    "        \n",
    "        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ì— ëŒ€í•´ì„œë§Œ classification_report ìƒì„±\n",
    "        intent_report = classification_report(\n",
    "            intent_labels_list, \n",
    "            intent_preds,\n",
    "            target_names=used_target_names, \n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        # NER í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "        ner_report = seqeval_report(ner_true, ner_preds, output_dict=True, zero_division=0)\n",
    "        val_ner_f1 = ner_report['weighted avg']['f1-score']\n",
    "        \n",
    "        # ì—í¬í¬ ê²°ê³¼ ìš”ì•½ \n",
    "        avg_f1 = (val_intent_f1 + val_ner_f1) / 2\n",
    "        \n",
    "        # ì§„í–‰ë°”ì— ì—í¬í¬ ê²°ê³¼ í‘œì‹œ\n",
    "        main_progress_bar.set_postfix({\n",
    "            'Ep': f\"{epoch+1}/{num_epochs}\",\n",
    "            'intent_acc': f\"{val_intent_accuracy:.4f}\",\n",
    "            'avg_f1': f\"{avg_f1:.4f}\"\n",
    "        })\n",
    "        main_progress_bar.update(1)  # ì—í¬í¬ ìš”ì•½ ë‹¨ê³„ ì—…ë°ì´íŠ¸\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            save_model(model, tokenizer, output_dir)\n",
    "            main_progress_bar.set_postfix({\n",
    "                'Ep': f\"{epoch+1}/{num_epochs}\",\n",
    "                'best_avg_f1': f\"{avg_f1:.4f}\",\n",
    "                'saved': 'Y'\n",
    "            })\n",
    "        \n",
    "        main_progress_bar.update(1)  # ëª¨ë¸ ì €ì¥ ë‹¨ê³„ ì—…ë°ì´íŠ¸\n",
    "    \n",
    "    main_progress_bar.close()\n",
    "    return best_f1\n",
    "\n",
    "# í›ˆë ¨ ì‹¤í–‰\n",
    "best_f1 = 0\n",
    "output_dir = \"./korean_library_chatbot_model\"\n",
    "logging_steps = 10  # 10ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸\n",
    "\n",
    "# í›ˆë ¨ ì‹œì‘\n",
    "print(\"\\ní›ˆë ¨ ì‹œì‘...\")\n",
    "best_f1 = train_and_evaluate(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=EPOCHS,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "print(f\"\\ní›ˆë ¨ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ì´ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ìµœê³  í‰ê·  F1: {best_f1:.4f})\")\n",
    "\n",
    "# ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ ê°œì„ \n",
    "def extract_entities_from_tokens(text, token_predictions, offset_mapping, id2tag):\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, (pred, (start, end)) in enumerate(zip(token_predictions, offset_mapping)):\n",
    "        if start == end:  # íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "            continue\n",
    "            \n",
    "        tag = id2tag[pred]\n",
    "        \n",
    "        if tag.startswith(\"B-\"):  # ì—”í‹°í‹° ì‹œì‘\n",
    "            # ì´ì „ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´ ì €ì¥\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            entity_type = tag[2:]  # \"B-\" ì œê±°\n",
    "            current_entity = {\n",
    "                \"type\": entity_type,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"text\": text[start:end]\n",
    "            }\n",
    "        \n",
    "        elif tag.startswith(\"I-\") and current_entity is not None:\n",
    "            # ì´ì „ ì—”í‹°í‹°ì™€ ê°™ì€ íƒ€ì…ì¸ ê²½ìš°ë§Œ í™•ì¥\n",
    "            if current_entity[\"type\"] == tag[2:]:\n",
    "                current_entity[\"end\"] = end\n",
    "                current_entity[\"text\"] = text[current_entity[\"start\"]:end]\n",
    "        \n",
    "        elif tag == \"O\":  # ì—”í‹°í‹° ì¢…ë£Œ\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì—”í‹°í‹° ì²˜ë¦¬\n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    # ì¤‘ë³µ ì—”í‹°í‹° ì œê±° (ë” ê¸´ ì—”í‹°í‹° ìš°ì„ )\n",
    "    filtered_entities = []\n",
    "    for entity in sorted(entities, key=lambda e: len(e[\"text\"]), reverse=True):\n",
    "        # ì´ë¯¸ í¬í•¨ëœ ì—”í‹°í‹°ì¸ì§€ í™•ì¸\n",
    "        is_contained = False\n",
    "        for filtered in filtered_entities:\n",
    "            if (entity[\"start\"] >= filtered[\"start\"] and \n",
    "                entity[\"end\"] <= filtered[\"end\"] and\n",
    "                entity[\"type\"] == filtered[\"type\"]):\n",
    "                is_contained = True\n",
    "                break\n",
    "        \n",
    "        if not is_contained:\n",
    "            filtered_entities.append(entity)\n",
    "    \n",
    "    return filtered_entities\n",
    "\n",
    "# ëª¨ë¸ ì¶”ë¡  í•¨ìˆ˜ ê°œì„ \n",
    "def predict_intent_and_entities(text, model, tokenizer, id2intent, id2tag):\n",
    "    model.eval()\n",
    "    \n",
    "    # í† í°í™” (ì˜¤í”„ì…‹ ë§¤í•‘ í¬í•¨)\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    offset_mapping = encoding[\"offset_mapping\"].squeeze().cpu().numpy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        intent_logits, ner_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Intent ì˜ˆì¸¡\n",
    "        intent_pred = torch.argmax(intent_logits, dim=1).cpu().numpy()[0]\n",
    "        intent_name = id2intent[intent_pred]\n",
    "        \n",
    "        # NER ì˜ˆì¸¡\n",
    "        ner_pred = torch.argmax(ner_logits, dim=2).cpu().numpy()[0]\n",
    "        \n",
    "        # í† í°í™”ëœ ê²°ê³¼ í™•ì¸ (ë””ë²„ê¹…ìš©)\n",
    "        print(\"\\ní† í°í™” ê²°ê³¼:\")\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "        for i, (token, pred) in enumerate(zip(tokens, ner_pred)):\n",
    "            if attention_mask[0][i] == 1:  # íŒ¨ë”©ì´ ì•„ë‹Œ í† í°ë§Œ\n",
    "                print(f\"  {token}: {id2tag[pred]}\")\n",
    "        \n",
    "        # ì‹¤ì œ í…ìŠ¤íŠ¸ ë²”ìœ„ì— ë§ê²Œ ì—”í‹°í‹° ì¶”ì¶œ\n",
    "        entities = extract_entities_from_tokens(text, ner_pred, offset_mapping, id2tag)\n",
    "    \n",
    "    return {\n",
    "        \"intent\": intent_name,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "def postprocess_prediction(text, prediction, common_authors_set=None, common_books_set=None):\n",
    "    current_intent = prediction[\"intent\"]\n",
    "    original_model_intent = prediction[\"intent\"] \n",
    "    entities = prediction[\"entities\"]\n",
    "    \n",
    "    has_author_entity = any(e[\"type\"] == \"AUTHOR\" for e in entities)\n",
    "    has_book_entity = any(e[\"type\"] == \"BOOK\" for e in entities)\n",
    "\n",
    "    # ì‹œë‚˜ë¦¬ì˜¤ 1: ì‘ê°€ ì—”í‹°í‹°ë§Œ ìˆê³ , ëª¨ë¸ì´ 'ë„ì„œê²€ìƒ‰'ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²½ìš° -> 'ì‘ê°€ê²€ìƒ‰'ìœ¼ë¡œ ë³´ì •\n",
    "    if original_model_intent == \"ë„ì„œê²€ìƒ‰\" and has_author_entity and not has_book_entity:\n",
    "        current_intent = \"ì‘ê°€ê²€ìƒ‰\" \n",
    "\n",
    "    # ì‹œë‚˜ë¦¬ì˜¤ 2: ì±… ì—”í‹°í‹°ë§Œ ìˆê³ , ëª¨ë¸ì´ 'ì‘ê°€ê²€ìƒ‰'ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²½ìš° -> 'ë„ì„œê²€ìƒ‰'ìœ¼ë¡œ ë³´ì •\n",
    "    elif original_model_intent == \"ì‘ê°€ê²€ìƒ‰\" and has_book_entity and not has_author_entity:\n",
    "        current_intent = \"ë„ì„œê²€ìƒ‰\"\n",
    "        \n",
    "    if original_model_intent != current_intent:\n",
    "        print(f\"INFO: Intent corrected by postprocessing. Original: '{original_model_intent}', Corrected: '{current_intent}'. Text: '{text}'\")\n",
    "\n",
    "    return {\n",
    "        \"intent\": current_intent,\n",
    "        \"entities\": entities,\n",
    "        \"original_intent\": original_model_intent,\n",
    "        \"text\": text  # í•¨ìˆ˜ì˜ ì¸ìë¡œ ë°›ì€ textë¥¼ ì‚¬ìš©\n",
    "    }\n",
    "\n",
    "# ê°œì„ ëœ ì¶”ë¡  ì‹¤í–‰\n",
    "print(\"\\nê°œì„ ëœ ì¶”ë¡  ì˜ˆì‹œ:\")\n",
    "sample_texts = [\n",
    "    \"ìŠ¤ì¦ˆë¯¸ì•¼ í•˜ë£¨íˆì˜ ìš°ìš¸ ë„ì„œê´€ì—ì— ìˆë‚˜ìš”?\",\n",
    "    \"ì „ìƒí–ˆë”ë‹ˆ ìŠ¬ë¼ì„ì´ì˜€ë˜ ê±´ì— ëŒ€í•˜ì—¬ ì±… ìˆì–´?\",\n",
    "    \"í•´ë¦¬í¬í„°ì™€ ë¶ˆì˜ ì” ì–´ë”” ìˆì–´?\",\n",
    "    \"ë°•ì™„ì„œ ì‘ê°€ì— ëŒ€í•´ ì•Œë ¤ì¤˜\"\n",
    "]\n",
    "\n",
    "for sample_text in sample_texts:\n",
    "    print(f\"\\nì…ë ¥ í…ìŠ¤íŠ¸: {sample_text}\")\n",
    "    \n",
    "    # ì›ë³¸ ì˜ˆì¸¡\n",
    "    raw_prediction = predict_intent_and_entities(\n",
    "        sample_text, model, tokenizer, id2intent, id2tag\n",
    "    )\n",
    "    \n",
    "    # í›„ì²˜ë¦¬ë¡œ ì˜ˆì¸¡ ê°œì„ \n",
    "    prediction = postprocess_prediction(sample_text, raw_prediction)\n",
    "    \n",
    "    print(f\"ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "    print(f\"  ì˜ë„: {prediction['intent']}\")\n",
    "    print(f\"  ê°œì²´:\")\n",
    "    if not prediction['entities']:\n",
    "        print(\"    - ê°œì²´ ì—†ìŒ\")\n",
    "    else:\n",
    "        for entity in prediction['entities']:\n",
    "            print(f\"    - {entity['type']}: '{entity['text']}' (ìœ„ì¹˜: {entity['start']}-{entity['end']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
