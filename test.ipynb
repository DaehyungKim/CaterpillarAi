{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a990e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af97a2e7a9b6483e98562040adfc6a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h2>ë„ì„œ ê²€ìƒ‰ NLU ëª¨ë¸ ì˜ˆì¸¡ ì¸í„°í˜ì´ìŠ¤</h2>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ee6e2880234d51ba63f76c7a36a385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>ëª¨ë¸ ë¡œë“œ ì¤‘...</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent ëª¨ë¸ ë¡œë“œ ì„±ê³µ: 40ê°œ ì˜ë„ í´ë˜ìŠ¤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta_crf to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/ner and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëˆ„ë½ëœ í‚¤: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: []\n",
      "NER ëª¨ë¸ ë¡œë“œ ì„±ê³µ: 51ê°œ NER íƒœê·¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a5769846d7459a8f1441235bf7042f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='ì§ˆë¬¸:', layout=Layout(width='80%'), placeholder='ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”'), Button(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb351a443394a80a619a47e3bbbe7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import commentjson\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "import traceback\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "from torchcrf import CRF\n",
    "from transformers import BertModel, RobertaModel\n",
    "\n",
    "# --- 0. ê¸°ë³¸ ì„¤ì • ---\n",
    "MODEL_NAME = \"klue/bert-base\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "INTENT_MODEL_DIR = \"./models/intent\"\n",
    "NER_MODEL_DIR = \"./models/ner\"\n",
    "INTENT_LABEL_PATH = os.path.join(INTENT_MODEL_DIR, \"intent_labels.jsonc\")\n",
    "NER_LABEL_PATH = os.path.join(NER_MODEL_DIR, \"ner_labels.jsonc\")\n",
    "\n",
    "GAZETTEER_DIR = \"./gazetteer\"\n",
    "BOOKS_GAZETTEER_PATH = os.path.join(GAZETTEER_DIR, \"titles.json\")\n",
    "AUTHORS_GAZETTEER_PATH = os.path.join(GAZETTEER_DIR, \"authors.json\")\n",
    "\n",
    "\n",
    "# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ ê´€ë ¨ ê°ì²´ ì„ ì–¸\n",
    "intent_model = None\n",
    "intent_tokenizer = None\n",
    "intent_id2label = None\n",
    "ner_model = None\n",
    "ner_tokenizer = None\n",
    "ner_id2label = None\n",
    "\n",
    "class RobertaCRF(nn.Module):\n",
    "    def __init__(self, model_dir, num_labels):\n",
    "        super(RobertaCRF, self).__init__()\n",
    "        # config ì§ì ‘ ë¡œë“œ ë° ìˆ˜ì •\n",
    "        from transformers import RobertaConfig\n",
    "        config_path = os.path.join(model_dir, \"config.json\")\n",
    "        if os.path.exists(config_path):\n",
    "            config = RobertaConfig.from_pretrained(config_path)\n",
    "            # Pooler ì‚¬ìš© ì•ˆí•¨ ì„¤ì •\n",
    "            config.add_pooling_layer = False\n",
    "        else:\n",
    "            config = RobertaConfig.from_pretrained(model_dir)\n",
    "            config.add_pooling_layer = False\n",
    "\n",
    "        # ìˆ˜ì •ëœ configë¡œ ëª¨ë¸ ë¡œë“œ\n",
    "        self.roberta = RobertaModel.from_pretrained(model_dir, config=config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]  # [batch_size, seq_len, hidden_size]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            # CRF ì†ì‹¤ ê³„ì‚°\n",
    "            mask = attention_mask.byte() if attention_mask is not None else None\n",
    "            loss = -self.crf(emissions, labels, mask=mask)\n",
    "            return type('CRFOutput', (), {'loss': loss, 'logits': emissions})()\n",
    "        else:\n",
    "            # ë””ì½”ë”© (ì˜ˆì¸¡)\n",
    "            mask = attention_mask.byte() if attention_mask is not None else None\n",
    "            tags = self.crf.decode(emissions, mask=mask)\n",
    "            return type('CRFOutput', (), {'tags': tags, 'logits': emissions})()\n",
    "\n",
    "def load_gazetteer():\n",
    "    # Gazetteer ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    os.makedirs(GAZETTEER_DIR, exist_ok=True)\n",
    "\n",
    "    # ì±… ëª©ë¡ Gazetteer\n",
    "    title = {}\n",
    "    try:\n",
    "        with open(\"gazetteers/titles.jsonc\", 'r', encoding='utf-8') as f:\n",
    "            loaded_data = json.load(f)\n",
    "            # ë°ì´í„° í˜•ì‹ í™•ì¸ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬)\n",
    "            if isinstance(loaded_data, dict):\n",
    "                title = loaded_data\n",
    "            else:  # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                title = loaded_data\n",
    "            print(f\"ì±… Gazetteer ë¡œë“œ ì™„ë£Œ: {len(title)} í•­ëª©\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì±… Gazetteer ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "        title = []\n",
    "\n",
    "    # ì‘ê°€ ëª©ë¡ Gazetteer\n",
    "    author = {}\n",
    "    try:\n",
    "        with open(\"gazetteers/authors.jsonc\", 'r', encoding='utf-8') as f:\n",
    "            loaded_data = json.load(f)\n",
    "            # ë°ì´í„° í˜•ì‹ í™•ì¸ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬)\n",
    "            if isinstance(loaded_data, dict):\n",
    "                author = loaded_data\n",
    "            else:  # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                author = loaded_data\n",
    "            print(f\"ì‘ê°€ Gazetteer ë¡œë“œ ì™„ë£Œ: {len(author)} í•­ëª©\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì‘ê°€ Gazetteer ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "        author = []\n",
    "\n",
    "    # ë°ì´í„° í˜•ì‹ ë¡œê¹…\n",
    "    print(f\"ì œëª© ë°ì´í„° í˜•ì‹: {type(title).__name__}\")\n",
    "    print(f\"ì‘ê°€ ë°ì´í„° í˜•ì‹: {type(author).__name__}\")\n",
    "\n",
    "    return {\"title\": title, \"author\": author}\n",
    "\n",
    "# --- 1. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ---\n",
    "def load_intent_model():\n",
    "    global intent_model, intent_tokenizer, intent_id2label\n",
    "\n",
    "    try:\n",
    "        # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "        intent_tokenizer = AutoTokenizer.from_pretrained(INTENT_MODEL_DIR)\n",
    "\n",
    "        # ë ˆì´ë¸” ì •ë³´ ë¡œë“œ\n",
    "        with open(INTENT_LABEL_PATH, 'r', encoding='utf-8') as f:\n",
    "            label_info = commentjson.load(f)\n",
    "\n",
    "        intent_id2label = {int(k): v for k, v in label_info[\"id2label\"].items()}\n",
    "        label2id = label_info[\"label2id\"]\n",
    "\n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        intent_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            INTENT_MODEL_DIR,\n",
    "            num_labels=len(intent_id2label),\n",
    "            id2label=intent_id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "\n",
    "        print(f\"Intent ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {len(intent_id2label)}ê°œ ì˜ë„ í´ë˜ìŠ¤\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Intent ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_ner_model():\n",
    "    global ner_model, ner_tokenizer, ner_id2label\n",
    "\n",
    "    try:\n",
    "        # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "        ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_DIR)\n",
    "\n",
    "        # ë ˆì´ë¸” ì •ë³´ ë¡œë“œ\n",
    "        with open(NER_LABEL_PATH, 'r', encoding='utf-8') as f:\n",
    "            label_info = commentjson.load(f)\n",
    "\n",
    "        ner_id2label = {int(k): v for k, v in label_info[\"id2label\"].items()}\n",
    "        label2id = label_info[\"label2id\"]\n",
    "\n",
    "        # ì»¤ìŠ¤í…€ ëª¨ë¸ ë¡œë“œ\n",
    "        ner_model = RobertaCRF(NER_MODEL_DIR, len(ner_id2label))\n",
    "\n",
    "        # ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ - strict=False ì ìš©\n",
    "        state_dict_path = os.path.join(NER_MODEL_DIR, \"pytorch_model.bin\")\n",
    "        if os.path.exists(state_dict_path):\n",
    "            state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "            # strict=Falseë¡œ ì„¤ì •í•˜ì—¬ ì¼ë¶€ íŒŒë¼ë¯¸í„°ê°€ ì—†ì–´ë„ ë¡œë“œë˜ë„ë¡ í•¨\n",
    "            missing_keys, unexpected_keys = ner_model.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"ëˆ„ë½ëœ í‚¤: {missing_keys}\")\n",
    "            print(f\"ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {unexpected_keys}\")\n",
    "\n",
    "        print(f\"NER ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {len(ner_id2label)}ê°œ NER íƒœê·¸\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"NER ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# --- 2. ì˜ˆì¸¡ í•¨ìˆ˜ ---\n",
    "def predict_intent(text):\n",
    "    global intent_model, intent_tokenizer, intent_id2label\n",
    "\n",
    "    if not intent_model or not intent_tokenizer or not intent_id2label:\n",
    "        return {\"intent\": \"ëª¨ë¸ ë¯¸ë¡œë“œ\", \"confidence\": 0.0}\n",
    "\n",
    "    intent_model.eval()\n",
    "\n",
    "    inputs = intent_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    intent_model = intent_model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = intent_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_intent = intent_id2label[predicted_class_id]\n",
    "\n",
    "    return {\n",
    "        \"intent\": predicted_intent,\n",
    "        \"confidence\": probabilities.cpu().numpy().flatten().tolist()\n",
    "    }\n",
    "\n",
    "def predict_entities(text, gazetteer=None):\n",
    "    global ner_model, ner_tokenizer, ner_id2label\n",
    "\n",
    "    if not ner_model or not ner_tokenizer or not ner_id2label:\n",
    "        return {\"tokens\": [], \"tags\": [], \"entities\": [], \"token_probabilities\": []}\n",
    "\n",
    "    ner_model.eval()\n",
    "\n",
    "    inputs = ner_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\").cpu().numpy()[0]\n",
    "\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ner_model = ner_model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = ner_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=2)\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][0].cpu().numpy()\n",
    "    tokens = ner_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    predicted_tag_ids = predictions[0].cpu().numpy()\n",
    "    all_probabilities = probabilities[0].cpu().numpy()\n",
    "\n",
    "    content_tokens = []\n",
    "    content_tags = []\n",
    "    token_probabilities_list = []\n",
    "    content_offsets = []\n",
    "\n",
    "    for i, (token, tag_id, input_id, (start, end)) in enumerate(zip(tokens, predicted_tag_ids, input_ids, offset_mapping)):\n",
    "        if input_id == ner_tokenizer.pad_token_id:\n",
    "            break\n",
    "\n",
    "        if token not in ner_tokenizer.all_special_tokens:\n",
    "            content_tokens.append(token)\n",
    "            tag = ner_id2label.get(tag_id, \"O\")\n",
    "            content_tags.append(tag)\n",
    "            content_offsets.append((start, end))\n",
    "            token_probabilities_list.append(all_probabilities[i].tolist())\n",
    "\n",
    "    # ì—”í‹°í‹° ì¶”ì¶œ (ëª¨ë¸ ì˜ˆì¸¡)\n",
    "    model_entities = []\n",
    "    current_entity_tokens = []\n",
    "    current_entity_starts = []\n",
    "    current_entity_ends = []\n",
    "    current_entity_label = None\n",
    "\n",
    "    for i, (token, tag, (start, end)) in enumerate(zip(content_tokens, content_tags, content_offsets)):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_entity_tokens:\n",
    "                entity_start_offset = current_entity_starts[0]\n",
    "                entity_end_offset = current_entity_ends[-1]\n",
    "                entity_text = text[entity_start_offset:entity_end_offset]\n",
    "                model_entities.append({\n",
    "                    \"entity\": entity_text,\n",
    "                    \"label\": current_entity_label,\n",
    "                    \"start\": entity_start_offset,\n",
    "                    \"end\": entity_end_offset,\n",
    "                    \"source\": \"model\",\n",
    "                    \"priority\": 1  # ëª¨ë¸ ì—”í‹°í‹°ì— ê¸°ë³¸ ìš°ì„ ìˆœìœ„ ë¶€ì—¬\n",
    "                })\n",
    "\n",
    "            current_entity_tokens = [token]\n",
    "            current_entity_starts = [start]\n",
    "            current_entity_ends = [end]\n",
    "            current_entity_label = tag[2:]\n",
    "\n",
    "        elif tag.startswith(\"I-\") and current_entity_label == tag[2:]:\n",
    "            current_entity_tokens.append(token)\n",
    "            current_entity_starts.append(start)\n",
    "            current_entity_ends.append(end)\n",
    "\n",
    "        else:\n",
    "            if current_entity_tokens:\n",
    "                entity_start_offset = current_entity_starts[0]\n",
    "                entity_end_offset = current_entity_ends[-1]\n",
    "                entity_text = text[entity_start_offset:entity_end_offset]\n",
    "                model_entities.append({\n",
    "                    \"entity\": entity_text,\n",
    "                    \"label\": current_entity_label,\n",
    "                    \"start\": entity_start_offset,\n",
    "                    \"end\": entity_end_offset,\n",
    "                    \"source\": \"model\",\n",
    "                    \"priority\": 1  # ëª¨ë¸ ì—”í‹°í‹°ì— ê¸°ë³¸ ìš°ì„ ìˆœìœ„ ë¶€ì—¬\n",
    "                })\n",
    "            current_entity_tokens = []\n",
    "            current_entity_starts = []\n",
    "            current_entity_ends = []\n",
    "            current_entity_label = None\n",
    "\n",
    "    if current_entity_tokens:\n",
    "        entity_start_offset = current_entity_starts[0]\n",
    "        entity_end_offset = current_entity_ends[-1]\n",
    "        entity_text = text[entity_start_offset:entity_end_offset]\n",
    "        model_entities.append({\n",
    "            \"entity\": entity_text,\n",
    "            \"label\": current_entity_label,\n",
    "            \"start\": entity_start_offset,\n",
    "            \"end\": entity_end_offset,\n",
    "            \"source\": \"model\",\n",
    "            \"priority\": 1  # ëª¨ë¸ ì—”í‹°í‹°ì— ê¸°ë³¸ ìš°ì„ ìˆœìœ„ ë¶€ì—¬\n",
    "        })\n",
    "\n",
    "    # === ê°€ì œí‹°ì–´ ì²˜ë¦¬ ê°œì„  ì‹œì‘ ===\n",
    "    gazetteer_entities = []\n",
    "    if gazetteer:\n",
    "        # ì‘ê°€(author) ê°€ì œí‹°ì–´ ë¨¼ì € ì²˜ë¦¬\n",
    "        if \"author\" in gazetteer and gazetteer[\"author\"]:\n",
    "            author_items = []\n",
    "            entity_data = gazetteer[\"author\"]\n",
    "            if isinstance(entity_data, list):\n",
    "                for item in entity_data:\n",
    "                    if isinstance(item, str):\n",
    "                        author_items.append(item)\n",
    "                    elif isinstance(item, dict) and 'name' in item:\n",
    "                        author_items.append(item['name'])\n",
    "            elif isinstance(entity_data, dict):\n",
    "                author_items.extend(entity_data.keys())\n",
    "\n",
    "            # ê¸¸ì´ ìˆœ ì •ë ¬ (ê¸´ ì‘ê°€ëª… ë¨¼ì €)\n",
    "            author_items.sort(key=len, reverse=True)\n",
    "\n",
    "            for author in author_items:\n",
    "                if not author or len(author) < 2:  # ë„ˆë¬´ ì§§ì€ ì‘ê°€ëª…ì€ ê±´ë„ˆë›°ê¸°\n",
    "                    continue\n",
    "\n",
    "                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì‘ê°€ëª… ì°¾ê¸°\n",
    "                start_pos = 0\n",
    "                while True:\n",
    "                    pos = text.lower().find(author.lower(), start_pos)\n",
    "                    if pos == -1:\n",
    "                        break\n",
    "                    end_pos = pos + len(author)\n",
    "                    exact_entity = text[pos:end_pos]\n",
    "\n",
    "                    gazetteer_entities.append({\n",
    "                        \"entity\": exact_entity,\n",
    "                        \"label\": \"author\",\n",
    "                        \"start\": pos,\n",
    "                        \"end\": end_pos,\n",
    "                        \"source\": \"gazetteer\",\n",
    "                        \"priority\": 3  # ì‘ê°€ ê°€ì œí‹°ì–´ì— ë†’ì€ ìš°ì„ ìˆœìœ„ ë¶€ì—¬\n",
    "                    })\n",
    "                    start_pos = end_pos\n",
    "\n",
    "        # ì œëª©(title) ê°€ì œí‹°ì–´ ì²˜ë¦¬\n",
    "        if \"title\" in gazetteer and gazetteer[\"title\"]:\n",
    "            title_items = []\n",
    "            entity_data = gazetteer[\"title\"]\n",
    "            if isinstance(entity_data, list):\n",
    "                for item in entity_data:\n",
    "                    if isinstance(item, str):\n",
    "                        title_items.append(item)\n",
    "                    elif isinstance(item, dict) and 'name' in item:\n",
    "                        title_items.append(item['name'])\n",
    "            elif isinstance(entity_data, dict):\n",
    "                title_items.extend(entity_data.keys())\n",
    "\n",
    "            # ê¸¸ì´ ìˆœ ì •ë ¬ (ê¸´ ì œëª© ë¨¼ì €)\n",
    "            title_items.sort(key=len, reverse=True)\n",
    "\n",
    "            for title in title_items:\n",
    "                if not title or len(title) < 2:  # ë„ˆë¬´ ì§§ì€ ì œëª©ì€ ê±´ë„ˆë›°ê¸°\n",
    "                    continue\n",
    "\n",
    "                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì œëª© ì°¾ê¸°\n",
    "                start_pos = 0\n",
    "                while True:\n",
    "                    pos = text.lower().find(title.lower(), start_pos)\n",
    "                    if pos == -1:\n",
    "                        break\n",
    "                    end_pos = pos + len(title)\n",
    "                    exact_entity = text[pos:end_pos]\n",
    "\n",
    "                    gazetteer_entities.append({\n",
    "                        \"entity\": exact_entity,\n",
    "                        \"label\": \"title\",\n",
    "                        \"start\": pos,\n",
    "                        \"end\": end_pos,\n",
    "                        \"source\": \"gazetteer\",\n",
    "                        \"priority\": 2  # ì œëª© ê°€ì œí‹°ì–´ì— ì¤‘ê°„ ìš°ì„ ìˆœìœ„ ë¶€ì—¬\n",
    "                    })\n",
    "                    start_pos = end_pos\n",
    "\n",
    "    # ëª¨ë¸ ì—”í‹°í‹°ì™€ ê°€ì œí‹°ì–´ ì—”í‹°í‹° í•©ì¹˜ê¸°\n",
    "    all_entities = model_entities + gazetteer_entities\n",
    "\n",
    "    # ìš°ì„ ìˆœìœ„ ì •ë ¬:\n",
    "    # 1. ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒë¶€í„° (ì‘ê°€ > ì œëª© > ëª¨ë¸)\n",
    "    # 2. ê°™ì€ ìš°ì„ ìˆœìœ„ë©´ ê¸´ ê²ƒ ë¨¼ì €\n",
    "    all_entities.sort(key=lambda x: (-x['priority'], -(x['end'] - x['start']), x['start']))\n",
    "\n",
    "    # ë²”ìœ„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ/í¬í•¨ ê´€ê³„ ì²˜ë¦¬\n",
    "    final_entities = []\n",
    "    covered_positions = set()  # ì´ë¯¸ ì²˜ë¦¬ëœ ìœ„ì¹˜ ì¶”ì \n",
    "\n",
    "    for entity in all_entities:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        label = entity['label']\n",
    "\n",
    "        # ì™„ì „íˆ ë™ì¼í•œ ì—”í‹°í‹°ì¸ì§€ í™•ì¸\n",
    "        is_duplicate = False\n",
    "        for existing in final_entities:\n",
    "            if (start == existing['start'] and\n",
    "                end == existing['end'] and\n",
    "                label == existing['label']):\n",
    "                is_duplicate = True\n",
    "                break\n",
    "\n",
    "        if is_duplicate:\n",
    "            continue\n",
    "\n",
    "        # íŠ¹ë³„ ì²˜ë¦¬: ì‘ê°€ ì—”í‹°í‹°ëŠ” ì œëª©ì— í¬í•¨ë˜ì–´ë„ ë³„ë„ë¡œ ì²˜ë¦¬\n",
    "        # ì œëª© ì•ˆì— ìˆëŠ” ì‘ê°€ ì´ë¦„ë„ ì¶”ì¶œí•˜ê¸° ìœ„í•¨\n",
    "        if label == \"author\":\n",
    "            # ì´ ì‘ê°€ ì—”í‹°í‹°ê°€ í¬í•¨ëœ ì œëª© ì—”í‹°í‹°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "            is_inside_title = False\n",
    "            for existing in final_entities:\n",
    "                if (existing['label'] == 'title' and\n",
    "                    start >= existing['start'] and\n",
    "                    end <= existing['end']):\n",
    "                    is_inside_title = True\n",
    "                    break\n",
    "\n",
    "            # ì‘ê°€ê°€ ì œëª© ì•ˆì— ìˆë”ë¼ë„ ì¶”ê°€\n",
    "            if not is_inside_title or True:  # í•­ìƒ ì‘ê°€ ì¶”ê°€\n",
    "                # í¬ì§€ì…˜ ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ ì„ì‹œ ì„¸íŠ¸\n",
    "                entity_positions = set(range(start, end))\n",
    "                # ì´ë¯¸ ì²˜ë¦¬ëœ ì‘ê°€ ìœ„ì¹˜ì™€ í¬ê²Œ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸\n",
    "                overlap_ratio = len(entity_positions.intersection(covered_positions)) / len(entity_positions) if entity_positions else 0\n",
    "\n",
    "                if overlap_ratio < 0.5:  # 50% ë¯¸ë§Œìœ¼ë¡œ ê²¹ì¹˜ë©´ ì¶”ê°€\n",
    "                    final_entities.append(entity)\n",
    "                    covered_positions.update(entity_positions)\n",
    "                continue\n",
    "\n",
    "        # ì¼ë°˜ ì—”í‹°í‹° ì¤‘ë³µ ì²˜ë¦¬ (ì‘ê°€ ì œì™¸)\n",
    "        entity_positions = set(range(start, end))\n",
    "        # ê¸°ì¡´ ì»¤ë²„ëœ ìœ„ì¹˜ì™€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ ê³„ì‚°\n",
    "        overlap_ratio = len(entity_positions.intersection(covered_positions)) / len(entity_positions) if entity_positions else 0\n",
    "\n",
    "        # 50% ë¯¸ë§Œìœ¼ë¡œ ê²¹ì¹˜ë©´ ì¶”ê°€\n",
    "        if overlap_ratio < 0.5:\n",
    "            final_entities.append(entity)\n",
    "            covered_positions.update(entity_positions)\n",
    "\n",
    "    # ìš°ì„ ìˆœìœ„ í•„ë“œ ì œê±° (ì¶œë ¥ì— ë¶ˆí•„ìš”)\n",
    "    for entity in final_entities:\n",
    "        if 'priority' in entity:\n",
    "            del entity['priority']\n",
    "\n",
    "    # === ê°€ì œí‹°ì–´ ì²˜ë¦¬ ê°œì„  ë ===\n",
    "\n",
    "    # ë¬¸ì ë‹¨ìœ„ ì‹œê°í™”ë¥¼ ìœ„í•œ íƒœê·¸ ë§¤í•‘\n",
    "    char_tags = [\"_\"] * len(text)\n",
    "    for entity in sorted(final_entities, key=lambda x: x['end'] - x['start'], reverse=True):\n",
    "        start_pos = entity[\"start\"]\n",
    "        end_pos = entity[\"end\"]\n",
    "        entity_type = entity[\"label\"]\n",
    "\n",
    "        # ì´ë¯¸ ë‹¤ë¥¸ (ë” ê¸´) ì—”í‹°í‹°ê°€ í‘œì‹œëœ ë¶€ë¶„ì€ ë®ì–´ì“°ì§€ ì•ŠìŒ\n",
    "        can_mark = True\n",
    "        for k in range(start_pos, end_pos):\n",
    "            if char_tags[k] != '_' and char_tags[k] != f\"B-{entity_type}\" and char_tags[k] != f\"I-{entity_type}\":\n",
    "                can_mark = False\n",
    "                break\n",
    "\n",
    "        if can_mark:\n",
    "            char_tags[start_pos] = f\"B-{entity_type}\"\n",
    "            for k in range(start_pos + 1, end_pos):\n",
    "                char_tags[k] = f\"I-{entity_type}\"\n",
    "\n",
    "    return {\n",
    "        \"tokens\": content_tokens,\n",
    "        \"tags\": content_tags,\n",
    "        \"entities\": final_entities,\n",
    "        \"token_probabilities\": token_probabilities_list,\n",
    "        \"char_tags\": char_tags\n",
    "    }\n",
    "\n",
    "# --- 3. í†µí•© ì˜ˆì¸¡ í•¨ìˆ˜ ---\n",
    "def predict(text):\n",
    "    global intent_model, intent_tokenizer, intent_id2label, ner_model, ner_tokenizer, ner_id2label\n",
    "\n",
    "    if not intent_model or not ner_model:\n",
    "        return {\"text\": text, \"intent\": \"ì˜¤ë¥˜: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\", \"confidence\": 0.0, \"entities\": [], \"error\": \"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\"}\n",
    "    try:\n",
    "        # ê°€ì œí‹°ì–´ ë¡œë“œ\n",
    "        gazetteer = load_gazetteer()\n",
    "\n",
    "        # ì˜ë„ ì˜ˆì¸¡\n",
    "        intent_result = predict_intent(text)\n",
    "\n",
    "        # ê°œì²´ëª… ì¸ì‹ (ê°€ì œí‹°ì–´ ì „ë‹¬)\n",
    "        ner_result = predict_entities(text, gazetteer)\n",
    "\n",
    "        # ë¬¸ìë³„ íƒœê·¸ ì‹œê°í™” ì¶”ê°€\n",
    "        char_tags = ner_result.get(\"char_tags\", [\"_\"] * len(text))\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"intent\": intent_result[\"intent\"],\n",
    "            \"confidence\": max(intent_result[\"confidence\"]) if intent_result.get(\"confidence\") else 0.0,\n",
    "            \"entities\": ner_result[\"entities\"],\n",
    "            \"ner_tokens\": ner_result.get(\"tokens\", []),\n",
    "            \"ner_token_probabilities\": ner_result.get(\"token_probabilities\", []),\n",
    "            \"char_tags\": \"\".join(char_tags)  # ë¬¸ì ë‹¨ìœ„ ì‹œê°í™”\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"text\": text, \"intent\": \"ì˜ˆì¸¡ ì˜¤ë¥˜\", \"confidence\": 0.0, \"entities\": [],\n",
    "                \"ner_tokens\": [], \"ner_token_probabilities\": [], \"error\": str(e)}\n",
    "\n",
    "# --- 4. UI ìƒì„± ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ ---\n",
    "def create_ui():\n",
    "    # ìœ„ì ¯ ìƒì„±\n",
    "    title = widgets.HTML(value=\"<h2>ë„ì„œ ê²€ìƒ‰ NLU ëª¨ë¸ ì˜ˆì¸¡ ì¸í„°í˜ì´ìŠ¤</h2>\")\n",
    "    text_widget = widgets.Text(\n",
    "        description='ì§ˆë¬¸:',\n",
    "        placeholder='ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    button = widgets.Button(\n",
    "        description='ì˜ˆì¸¡',\n",
    "        button_style='primary',\n",
    "        tooltip='ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ ì‹¤í–‰'\n",
    "    )\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì²˜ë¦¬\n",
    "    def on_button_click(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            user_input = text_widget.value\n",
    "            if not user_input:\n",
    "                print(\"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "                return\n",
    "\n",
    "            print(f\"ì…ë ¥: \\\"{user_input}\\\"\")\n",
    "            print(\"ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\")\n",
    "            result = predict(user_input)\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            if \"error\" in result:\n",
    "                print(f\"ğŸš¨ ì˜ˆì¸¡ ì˜¤ë¥˜: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: \\\"{result['text']}\\\"\")\n",
    "                print(f\"ğŸ¯ ì˜ë„ ë¶„ì„ ê²°ê³¼: {result['intent']} (í™•ë¥ : {result.get('confidence', 0.0):.4f})\")\n",
    "\n",
    "                print(\"\\nğŸ” ê°œì²´ëª… ì¸ì‹ ê²°ê³¼:\")\n",
    "                if result.get('entities'):\n",
    "                    for i, entity in enumerate(result['entities'], 1):\n",
    "                        print(f\"  {i}. \\\"{entity['entity']}\\\" â†’ {entity['label']}\")\n",
    "                else:\n",
    "                    print(\"  ì¸ì‹ëœ ê°œì²´ëª… ì—†ìŒ\")\n",
    "\n",
    "                print(\"\\nğŸ“Š í† í°ë³„ NER í™•ë¥ :\")\n",
    "                if result.get('ner_tokens') and result.get('ner_token_probabilities'):\n",
    "                    if ner_id2label:\n",
    "                        for token, probs in zip(result['ner_tokens'], result['ner_token_probabilities']):\n",
    "                            print(f\"  - í† í°: '{token}'\")\n",
    "                            sorted_probs = sorted(enumerate(probs), key=lambda item: item[1], reverse=True)\n",
    "                            # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ\n",
    "                            for tag_id, prob in sorted_probs[:3]:\n",
    "                                print(f\"      {ner_id2label.get(tag_id, f'ID_{tag_id}')}: {prob:.4f}\")\n",
    "                    else:\n",
    "                        print(\"  (ì˜¤ë¥˜: NER ë ˆì´ë¸” ë§¤í•‘ ì •ë³´(ner_id2label)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)\")\n",
    "                else:\n",
    "                    print(\"  í† í°ë³„ í™•ë¥  ì •ë³´ ì—†ìŒ.\")\n",
    "\n",
    "                print(\"\\nğŸ“ ë¬¸ìë³„ NER íƒœê·¸ ì‹œê°í™”:\")\n",
    "                print(f\"  ì›ë¬¸: {result['text']}\")\n",
    "\n",
    "                text = result['text']\n",
    "                tag_markers = ['_'] * len(text)\n",
    "                processed_indices = set()\n",
    "\n",
    "                for entity in sorted(result.get('entities', []), key=lambda x: len(x['entity']), reverse=True):\n",
    "                    entity_text = entity['entity']\n",
    "                    entity_type = entity['label']\n",
    "                    start_pos = -1\n",
    "                    search_start = 0\n",
    "\n",
    "                    while True:\n",
    "                        temp_pos = text.find(entity_text, search_start)\n",
    "                        if temp_pos == -1: break\n",
    "\n",
    "                        is_overlapping = False\n",
    "                        for i in range(temp_pos, temp_pos + len(entity_text)):\n",
    "                            if i in processed_indices:\n",
    "                                is_overlapping = True\n",
    "                                break\n",
    "\n",
    "                        if not is_overlapping:\n",
    "                            start_pos = temp_pos\n",
    "                            break\n",
    "\n",
    "                        search_start = temp_pos + 1\n",
    "\n",
    "                    if start_pos != -1:\n",
    "                        end_pos = start_pos + len(entity_text)\n",
    "                        for i in range(start_pos, end_pos):\n",
    "                            if i not in processed_indices:\n",
    "                                tag_markers[i] = '^'\n",
    "                                processed_indices.add(i)\n",
    "\n",
    "                        tag_label = f\"[{entity_type}]\"\n",
    "                        for i, char in enumerate(tag_label):\n",
    "                            pos = start_pos + i\n",
    "                            if pos < len(tag_markers) and tag_markers[pos] == '^':\n",
    "                                tag_markers[pos] = char\n",
    "\n",
    "                print(f\"  íƒœê·¸: {''.join(tag_markers)}\")\n",
    "\n",
    "    # ì´ë²¤íŠ¸ ì—°ê²°\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # ì—”í„°í‚¤ë¡œë„ ì˜ˆì¸¡ ì‹¤í–‰\n",
    "    def on_enter(widget):\n",
    "        button.click()\n",
    "\n",
    "    text_widget.on_submit(on_enter)\n",
    "\n",
    "    # UI í‘œì‹œ\n",
    "    display(title)\n",
    "\n",
    "    model_status = widgets.HTML(value=\"<h3>ëª¨ë¸ ë¡œë“œ ì¤‘...</h3>\")\n",
    "    display(model_status)\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    intent_loaded = load_intent_model()\n",
    "    ner_loaded = load_ner_model()\n",
    "\n",
    "    if intent_loaded and ner_loaded:\n",
    "        model_status.value = \"<h3 style='color:green'>âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ</h3>\"\n",
    "    else:\n",
    "        model_status.value = \"<h3 style='color:red'>âœ— ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨</h3>\"\n",
    "\n",
    "    # ì…ë ¥ UI í‘œì‹œ\n",
    "    display(widgets.HBox([text_widget, button]))\n",
    "    display(output)\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    create_ui()\n",
    "else:\n",
    "    # Jupyterì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ê²½ìš°\n",
    "    create_ui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
