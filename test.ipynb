{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a990e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af97a2e7a9b6483e98562040adfc6a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h2>도서 검색 NLU 모델 예측 인터페이스</h2>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ee6e2880234d51ba63f76c7a36a385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>모델 로드 중...</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent 모델 로드 성공: 40개 의도 클래스\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta_crf to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/ner and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "누락된 키: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "예상치 못한 키: []\n",
      "NER 모델 로드 성공: 51개 NER 태그\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a5769846d7459a8f1441235bf7042f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='질문:', layout=Layout(width='80%'), placeholder='질문을 입력하세요'), Button(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb351a443394a80a619a47e3bbbe7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import commentjson\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "import traceback\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "from torchcrf import CRF\n",
    "from transformers import BertModel, RobertaModel\n",
    "\n",
    "# --- 0. 기본 설정 ---\n",
    "MODEL_NAME = \"klue/bert-base\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 모델 저장 경로 설정\n",
    "INTENT_MODEL_DIR = \"./models/intent\"\n",
    "NER_MODEL_DIR = \"./models/ner\"\n",
    "INTENT_LABEL_PATH = os.path.join(INTENT_MODEL_DIR, \"intent_labels.jsonc\")\n",
    "NER_LABEL_PATH = os.path.join(NER_MODEL_DIR, \"ner_labels.jsonc\")\n",
    "\n",
    "GAZETTEER_DIR = \"./gazetteer\"\n",
    "BOOKS_GAZETTEER_PATH = os.path.join(GAZETTEER_DIR, \"titles.json\")\n",
    "AUTHORS_GAZETTEER_PATH = os.path.join(GAZETTEER_DIR, \"authors.json\")\n",
    "\n",
    "\n",
    "# 전역 변수로 모델과 관련 객체 선언\n",
    "intent_model = None\n",
    "intent_tokenizer = None\n",
    "intent_id2label = None\n",
    "ner_model = None\n",
    "ner_tokenizer = None\n",
    "ner_id2label = None\n",
    "\n",
    "class RobertaCRF(nn.Module):\n",
    "    def __init__(self, model_dir, num_labels):\n",
    "        super(RobertaCRF, self).__init__()\n",
    "        # config 직접 로드 및 수정\n",
    "        from transformers import RobertaConfig\n",
    "        config_path = os.path.join(model_dir, \"config.json\")\n",
    "        if os.path.exists(config_path):\n",
    "            config = RobertaConfig.from_pretrained(config_path)\n",
    "            # Pooler 사용 안함 설정\n",
    "            config.add_pooling_layer = False\n",
    "        else:\n",
    "            config = RobertaConfig.from_pretrained(model_dir)\n",
    "            config.add_pooling_layer = False\n",
    "\n",
    "        # 수정된 config로 모델 로드\n",
    "        self.roberta = RobertaModel.from_pretrained(model_dir, config=config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]  # [batch_size, seq_len, hidden_size]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            # CRF 손실 계산\n",
    "            mask = attention_mask.byte() if attention_mask is not None else None\n",
    "            loss = -self.crf(emissions, labels, mask=mask)\n",
    "            return type('CRFOutput', (), {'loss': loss, 'logits': emissions})()\n",
    "        else:\n",
    "            # 디코딩 (예측)\n",
    "            mask = attention_mask.byte() if attention_mask is not None else None\n",
    "            tags = self.crf.decode(emissions, mask=mask)\n",
    "            return type('CRFOutput', (), {'tags': tags, 'logits': emissions})()\n",
    "\n",
    "def load_gazetteer():\n",
    "    # Gazetteer 디렉토리 생성\n",
    "    os.makedirs(GAZETTEER_DIR, exist_ok=True)\n",
    "\n",
    "    # 책 목록 Gazetteer\n",
    "    title = {}\n",
    "    try:\n",
    "        with open(\"gazetteers/titles.jsonc\", 'r', encoding='utf-8') as f:\n",
    "            loaded_data = json.load(f)\n",
    "            # 데이터 형식 확인 (리스트 또는 딕셔너리)\n",
    "            if isinstance(loaded_data, dict):\n",
    "                title = loaded_data\n",
    "            else:  # 리스트인 경우 그대로 사용\n",
    "                title = loaded_data\n",
    "            print(f\"책 Gazetteer 로드 완료: {len(title)} 항목\")\n",
    "    except Exception as e:\n",
    "        print(f\"책 Gazetteer 로드 오류: {e}\")\n",
    "        title = []\n",
    "\n",
    "    # 작가 목록 Gazetteer\n",
    "    author = {}\n",
    "    try:\n",
    "        with open(\"gazetteers/authors.jsonc\", 'r', encoding='utf-8') as f:\n",
    "            loaded_data = json.load(f)\n",
    "            # 데이터 형식 확인 (리스트 또는 딕셔너리)\n",
    "            if isinstance(loaded_data, dict):\n",
    "                author = loaded_data\n",
    "            else:  # 리스트인 경우 그대로 사용\n",
    "                author = loaded_data\n",
    "            print(f\"작가 Gazetteer 로드 완료: {len(author)} 항목\")\n",
    "    except Exception as e:\n",
    "        print(f\"작가 Gazetteer 로드 오류: {e}\")\n",
    "        author = []\n",
    "\n",
    "    # 데이터 형식 로깅\n",
    "    print(f\"제목 데이터 형식: {type(title).__name__}\")\n",
    "    print(f\"작가 데이터 형식: {type(author).__name__}\")\n",
    "\n",
    "    return {\"title\": title, \"author\": author}\n",
    "\n",
    "# --- 1. 모델 로드 함수 ---\n",
    "def load_intent_model():\n",
    "    global intent_model, intent_tokenizer, intent_id2label\n",
    "\n",
    "    try:\n",
    "        # 토크나이저 로드\n",
    "        intent_tokenizer = AutoTokenizer.from_pretrained(INTENT_MODEL_DIR)\n",
    "\n",
    "        # 레이블 정보 로드\n",
    "        with open(INTENT_LABEL_PATH, 'r', encoding='utf-8') as f:\n",
    "            label_info = commentjson.load(f)\n",
    "\n",
    "        intent_id2label = {int(k): v for k, v in label_info[\"id2label\"].items()}\n",
    "        label2id = label_info[\"label2id\"]\n",
    "\n",
    "        # 모델 로드\n",
    "        intent_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            INTENT_MODEL_DIR,\n",
    "            num_labels=len(intent_id2label),\n",
    "            id2label=intent_id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "\n",
    "        print(f\"Intent 모델 로드 성공: {len(intent_id2label)}개 의도 클래스\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Intent 모델 로드 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_ner_model():\n",
    "    global ner_model, ner_tokenizer, ner_id2label\n",
    "\n",
    "    try:\n",
    "        # 토크나이저 로드\n",
    "        ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_DIR)\n",
    "\n",
    "        # 레이블 정보 로드\n",
    "        with open(NER_LABEL_PATH, 'r', encoding='utf-8') as f:\n",
    "            label_info = commentjson.load(f)\n",
    "\n",
    "        ner_id2label = {int(k): v for k, v in label_info[\"id2label\"].items()}\n",
    "        label2id = label_info[\"label2id\"]\n",
    "\n",
    "        # 커스텀 모델 로드\n",
    "        ner_model = RobertaCRF(NER_MODEL_DIR, len(ner_id2label))\n",
    "\n",
    "        # 가중치 파일 로드 - strict=False 적용\n",
    "        state_dict_path = os.path.join(NER_MODEL_DIR, \"pytorch_model.bin\")\n",
    "        if os.path.exists(state_dict_path):\n",
    "            state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "            # strict=False로 설정하여 일부 파라미터가 없어도 로드되도록 함\n",
    "            missing_keys, unexpected_keys = ner_model.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"누락된 키: {missing_keys}\")\n",
    "            print(f\"예상치 못한 키: {unexpected_keys}\")\n",
    "\n",
    "        print(f\"NER 모델 로드 성공: {len(ner_id2label)}개 NER 태그\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"NER 모델 로드 실패: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# --- 2. 예측 함수 ---\n",
    "def predict_intent(text):\n",
    "    global intent_model, intent_tokenizer, intent_id2label\n",
    "\n",
    "    if not intent_model or not intent_tokenizer or not intent_id2label:\n",
    "        return {\"intent\": \"모델 미로드\", \"confidence\": 0.0}\n",
    "\n",
    "    intent_model.eval()\n",
    "\n",
    "    inputs = intent_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    intent_model = intent_model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = intent_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_intent = intent_id2label[predicted_class_id]\n",
    "\n",
    "    return {\n",
    "        \"intent\": predicted_intent,\n",
    "        \"confidence\": probabilities.cpu().numpy().flatten().tolist()\n",
    "    }\n",
    "\n",
    "def predict_entities(text, gazetteer=None):\n",
    "    global ner_model, ner_tokenizer, ner_id2label\n",
    "\n",
    "    if not ner_model or not ner_tokenizer or not ner_id2label:\n",
    "        return {\"tokens\": [], \"tags\": [], \"entities\": [], \"token_probabilities\": []}\n",
    "\n",
    "    ner_model.eval()\n",
    "\n",
    "    inputs = ner_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\").cpu().numpy()[0]\n",
    "\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ner_model = ner_model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = ner_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=2)\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][0].cpu().numpy()\n",
    "    tokens = ner_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    predicted_tag_ids = predictions[0].cpu().numpy()\n",
    "    all_probabilities = probabilities[0].cpu().numpy()\n",
    "\n",
    "    content_tokens = []\n",
    "    content_tags = []\n",
    "    token_probabilities_list = []\n",
    "    content_offsets = []\n",
    "\n",
    "    for i, (token, tag_id, input_id, (start, end)) in enumerate(zip(tokens, predicted_tag_ids, input_ids, offset_mapping)):\n",
    "        if input_id == ner_tokenizer.pad_token_id:\n",
    "            break\n",
    "\n",
    "        if token not in ner_tokenizer.all_special_tokens:\n",
    "            content_tokens.append(token)\n",
    "            tag = ner_id2label.get(tag_id, \"O\")\n",
    "            content_tags.append(tag)\n",
    "            content_offsets.append((start, end))\n",
    "            token_probabilities_list.append(all_probabilities[i].tolist())\n",
    "\n",
    "    # 엔티티 추출 (모델 예측)\n",
    "    model_entities = []\n",
    "    current_entity_tokens = []\n",
    "    current_entity_starts = []\n",
    "    current_entity_ends = []\n",
    "    current_entity_label = None\n",
    "\n",
    "    for i, (token, tag, (start, end)) in enumerate(zip(content_tokens, content_tags, content_offsets)):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_entity_tokens:\n",
    "                entity_start_offset = current_entity_starts[0]\n",
    "                entity_end_offset = current_entity_ends[-1]\n",
    "                entity_text = text[entity_start_offset:entity_end_offset]\n",
    "                model_entities.append({\n",
    "                    \"entity\": entity_text,\n",
    "                    \"label\": current_entity_label,\n",
    "                    \"start\": entity_start_offset,\n",
    "                    \"end\": entity_end_offset,\n",
    "                    \"source\": \"model\",\n",
    "                    \"priority\": 1  # 모델 엔티티에 기본 우선순위 부여\n",
    "                })\n",
    "\n",
    "            current_entity_tokens = [token]\n",
    "            current_entity_starts = [start]\n",
    "            current_entity_ends = [end]\n",
    "            current_entity_label = tag[2:]\n",
    "\n",
    "        elif tag.startswith(\"I-\") and current_entity_label == tag[2:]:\n",
    "            current_entity_tokens.append(token)\n",
    "            current_entity_starts.append(start)\n",
    "            current_entity_ends.append(end)\n",
    "\n",
    "        else:\n",
    "            if current_entity_tokens:\n",
    "                entity_start_offset = current_entity_starts[0]\n",
    "                entity_end_offset = current_entity_ends[-1]\n",
    "                entity_text = text[entity_start_offset:entity_end_offset]\n",
    "                model_entities.append({\n",
    "                    \"entity\": entity_text,\n",
    "                    \"label\": current_entity_label,\n",
    "                    \"start\": entity_start_offset,\n",
    "                    \"end\": entity_end_offset,\n",
    "                    \"source\": \"model\",\n",
    "                    \"priority\": 1  # 모델 엔티티에 기본 우선순위 부여\n",
    "                })\n",
    "            current_entity_tokens = []\n",
    "            current_entity_starts = []\n",
    "            current_entity_ends = []\n",
    "            current_entity_label = None\n",
    "\n",
    "    if current_entity_tokens:\n",
    "        entity_start_offset = current_entity_starts[0]\n",
    "        entity_end_offset = current_entity_ends[-1]\n",
    "        entity_text = text[entity_start_offset:entity_end_offset]\n",
    "        model_entities.append({\n",
    "            \"entity\": entity_text,\n",
    "            \"label\": current_entity_label,\n",
    "            \"start\": entity_start_offset,\n",
    "            \"end\": entity_end_offset,\n",
    "            \"source\": \"model\",\n",
    "            \"priority\": 1  # 모델 엔티티에 기본 우선순위 부여\n",
    "        })\n",
    "\n",
    "    # === 가제티어 처리 개선 시작 ===\n",
    "    gazetteer_entities = []\n",
    "    if gazetteer:\n",
    "        # 작가(author) 가제티어 먼저 처리\n",
    "        if \"author\" in gazetteer and gazetteer[\"author\"]:\n",
    "            author_items = []\n",
    "            entity_data = gazetteer[\"author\"]\n",
    "            if isinstance(entity_data, list):\n",
    "                for item in entity_data:\n",
    "                    if isinstance(item, str):\n",
    "                        author_items.append(item)\n",
    "                    elif isinstance(item, dict) and 'name' in item:\n",
    "                        author_items.append(item['name'])\n",
    "            elif isinstance(entity_data, dict):\n",
    "                author_items.extend(entity_data.keys())\n",
    "\n",
    "            # 길이 순 정렬 (긴 작가명 먼저)\n",
    "            author_items.sort(key=len, reverse=True)\n",
    "\n",
    "            for author in author_items:\n",
    "                if not author or len(author) < 2:  # 너무 짧은 작가명은 건너뛰기\n",
    "                    continue\n",
    "\n",
    "                # 대소문자 무시하고 작가명 찾기\n",
    "                start_pos = 0\n",
    "                while True:\n",
    "                    pos = text.lower().find(author.lower(), start_pos)\n",
    "                    if pos == -1:\n",
    "                        break\n",
    "                    end_pos = pos + len(author)\n",
    "                    exact_entity = text[pos:end_pos]\n",
    "\n",
    "                    gazetteer_entities.append({\n",
    "                        \"entity\": exact_entity,\n",
    "                        \"label\": \"author\",\n",
    "                        \"start\": pos,\n",
    "                        \"end\": end_pos,\n",
    "                        \"source\": \"gazetteer\",\n",
    "                        \"priority\": 3  # 작가 가제티어에 높은 우선순위 부여\n",
    "                    })\n",
    "                    start_pos = end_pos\n",
    "\n",
    "        # 제목(title) 가제티어 처리\n",
    "        if \"title\" in gazetteer and gazetteer[\"title\"]:\n",
    "            title_items = []\n",
    "            entity_data = gazetteer[\"title\"]\n",
    "            if isinstance(entity_data, list):\n",
    "                for item in entity_data:\n",
    "                    if isinstance(item, str):\n",
    "                        title_items.append(item)\n",
    "                    elif isinstance(item, dict) and 'name' in item:\n",
    "                        title_items.append(item['name'])\n",
    "            elif isinstance(entity_data, dict):\n",
    "                title_items.extend(entity_data.keys())\n",
    "\n",
    "            # 길이 순 정렬 (긴 제목 먼저)\n",
    "            title_items.sort(key=len, reverse=True)\n",
    "\n",
    "            for title in title_items:\n",
    "                if not title or len(title) < 2:  # 너무 짧은 제목은 건너뛰기\n",
    "                    continue\n",
    "\n",
    "                # 대소문자 무시하고 제목 찾기\n",
    "                start_pos = 0\n",
    "                while True:\n",
    "                    pos = text.lower().find(title.lower(), start_pos)\n",
    "                    if pos == -1:\n",
    "                        break\n",
    "                    end_pos = pos + len(title)\n",
    "                    exact_entity = text[pos:end_pos]\n",
    "\n",
    "                    gazetteer_entities.append({\n",
    "                        \"entity\": exact_entity,\n",
    "                        \"label\": \"title\",\n",
    "                        \"start\": pos,\n",
    "                        \"end\": end_pos,\n",
    "                        \"source\": \"gazetteer\",\n",
    "                        \"priority\": 2  # 제목 가제티어에 중간 우선순위 부여\n",
    "                    })\n",
    "                    start_pos = end_pos\n",
    "\n",
    "    # 모델 엔티티와 가제티어 엔티티 합치기\n",
    "    all_entities = model_entities + gazetteer_entities\n",
    "\n",
    "    # 우선순위 정렬:\n",
    "    # 1. 우선순위 높은 것부터 (작가 > 제목 > 모델)\n",
    "    # 2. 같은 우선순위면 긴 것 먼저\n",
    "    all_entities.sort(key=lambda x: (-x['priority'], -(x['end'] - x['start']), x['start']))\n",
    "\n",
    "    # 범위 기반으로 중복/포함 관계 처리\n",
    "    final_entities = []\n",
    "    covered_positions = set()  # 이미 처리된 위치 추적\n",
    "\n",
    "    for entity in all_entities:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        label = entity['label']\n",
    "\n",
    "        # 완전히 동일한 엔티티인지 확인\n",
    "        is_duplicate = False\n",
    "        for existing in final_entities:\n",
    "            if (start == existing['start'] and\n",
    "                end == existing['end'] and\n",
    "                label == existing['label']):\n",
    "                is_duplicate = True\n",
    "                break\n",
    "\n",
    "        if is_duplicate:\n",
    "            continue\n",
    "\n",
    "        # 특별 처리: 작가 엔티티는 제목에 포함되어도 별도로 처리\n",
    "        # 제목 안에 있는 작가 이름도 추출하기 위함\n",
    "        if label == \"author\":\n",
    "            # 이 작가 엔티티가 포함된 제목 엔티티가 있는지 확인\n",
    "            is_inside_title = False\n",
    "            for existing in final_entities:\n",
    "                if (existing['label'] == 'title' and\n",
    "                    start >= existing['start'] and\n",
    "                    end <= existing['end']):\n",
    "                    is_inside_title = True\n",
    "                    break\n",
    "\n",
    "            # 작가가 제목 안에 있더라도 추가\n",
    "            if not is_inside_title or True:  # 항상 작가 추가\n",
    "                # 포지션 중복 검사를 위한 임시 세트\n",
    "                entity_positions = set(range(start, end))\n",
    "                # 이미 처리된 작가 위치와 크게 겹치는지 확인\n",
    "                overlap_ratio = len(entity_positions.intersection(covered_positions)) / len(entity_positions) if entity_positions else 0\n",
    "\n",
    "                if overlap_ratio < 0.5:  # 50% 미만으로 겹치면 추가\n",
    "                    final_entities.append(entity)\n",
    "                    covered_positions.update(entity_positions)\n",
    "                continue\n",
    "\n",
    "        # 일반 엔티티 중복 처리 (작가 제외)\n",
    "        entity_positions = set(range(start, end))\n",
    "        # 기존 커버된 위치와 얼마나 겹치는지 계산\n",
    "        overlap_ratio = len(entity_positions.intersection(covered_positions)) / len(entity_positions) if entity_positions else 0\n",
    "\n",
    "        # 50% 미만으로 겹치면 추가\n",
    "        if overlap_ratio < 0.5:\n",
    "            final_entities.append(entity)\n",
    "            covered_positions.update(entity_positions)\n",
    "\n",
    "    # 우선순위 필드 제거 (출력에 불필요)\n",
    "    for entity in final_entities:\n",
    "        if 'priority' in entity:\n",
    "            del entity['priority']\n",
    "\n",
    "    # === 가제티어 처리 개선 끝 ===\n",
    "\n",
    "    # 문자 단위 시각화를 위한 태그 매핑\n",
    "    char_tags = [\"_\"] * len(text)\n",
    "    for entity in sorted(final_entities, key=lambda x: x['end'] - x['start'], reverse=True):\n",
    "        start_pos = entity[\"start\"]\n",
    "        end_pos = entity[\"end\"]\n",
    "        entity_type = entity[\"label\"]\n",
    "\n",
    "        # 이미 다른 (더 긴) 엔티티가 표시된 부분은 덮어쓰지 않음\n",
    "        can_mark = True\n",
    "        for k in range(start_pos, end_pos):\n",
    "            if char_tags[k] != '_' and char_tags[k] != f\"B-{entity_type}\" and char_tags[k] != f\"I-{entity_type}\":\n",
    "                can_mark = False\n",
    "                break\n",
    "\n",
    "        if can_mark:\n",
    "            char_tags[start_pos] = f\"B-{entity_type}\"\n",
    "            for k in range(start_pos + 1, end_pos):\n",
    "                char_tags[k] = f\"I-{entity_type}\"\n",
    "\n",
    "    return {\n",
    "        \"tokens\": content_tokens,\n",
    "        \"tags\": content_tags,\n",
    "        \"entities\": final_entities,\n",
    "        \"token_probabilities\": token_probabilities_list,\n",
    "        \"char_tags\": char_tags\n",
    "    }\n",
    "\n",
    "# --- 3. 통합 예측 함수 ---\n",
    "def predict(text):\n",
    "    global intent_model, intent_tokenizer, intent_id2label, ner_model, ner_tokenizer, ner_id2label\n",
    "\n",
    "    if not intent_model or not ner_model:\n",
    "        return {\"text\": text, \"intent\": \"오류: 모델 로드 실패\", \"confidence\": 0.0, \"entities\": [], \"error\": \"모델 로드 실패\"}\n",
    "    try:\n",
    "        # 가제티어 로드\n",
    "        gazetteer = load_gazetteer()\n",
    "\n",
    "        # 의도 예측\n",
    "        intent_result = predict_intent(text)\n",
    "\n",
    "        # 개체명 인식 (가제티어 전달)\n",
    "        ner_result = predict_entities(text, gazetteer)\n",
    "\n",
    "        # 문자별 태그 시각화 추가\n",
    "        char_tags = ner_result.get(\"char_tags\", [\"_\"] * len(text))\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"intent\": intent_result[\"intent\"],\n",
    "            \"confidence\": max(intent_result[\"confidence\"]) if intent_result.get(\"confidence\") else 0.0,\n",
    "            \"entities\": ner_result[\"entities\"],\n",
    "            \"ner_tokens\": ner_result.get(\"tokens\", []),\n",
    "            \"ner_token_probabilities\": ner_result.get(\"token_probabilities\", []),\n",
    "            \"char_tags\": \"\".join(char_tags)  # 문자 단위 시각화\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"예측 중 오류 발생: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"text\": text, \"intent\": \"예측 오류\", \"confidence\": 0.0, \"entities\": [],\n",
    "                \"ner_tokens\": [], \"ner_token_probabilities\": [], \"error\": str(e)}\n",
    "\n",
    "# --- 4. UI 생성 및 이벤트 처리 ---\n",
    "def create_ui():\n",
    "    # 위젯 생성\n",
    "    title = widgets.HTML(value=\"<h2>도서 검색 NLU 모델 예측 인터페이스</h2>\")\n",
    "    text_widget = widgets.Text(\n",
    "        description='질문:',\n",
    "        placeholder='질문을 입력하세요',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    button = widgets.Button(\n",
    "        description='예측',\n",
    "        button_style='primary',\n",
    "        tooltip='입력된 텍스트에 대해 예측 실행'\n",
    "    )\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # 버튼 클릭 이벤트 처리\n",
    "    def on_button_click(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            user_input = text_widget.value\n",
    "            if not user_input:\n",
    "                print(\"질문을 입력해주세요.\")\n",
    "                return\n",
    "\n",
    "            print(f\"입력: \\\"{user_input}\\\"\")\n",
    "            print(\"예측 수행 중...\")\n",
    "            result = predict(user_input)\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            if \"error\" in result:\n",
    "                print(f\"🚨 예측 오류: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"📄 입력 텍스트: \\\"{result['text']}\\\"\")\n",
    "                print(f\"🎯 의도 분석 결과: {result['intent']} (확률: {result.get('confidence', 0.0):.4f})\")\n",
    "\n",
    "                print(\"\\n🔍 개체명 인식 결과:\")\n",
    "                if result.get('entities'):\n",
    "                    for i, entity in enumerate(result['entities'], 1):\n",
    "                        print(f\"  {i}. \\\"{entity['entity']}\\\" → {entity['label']}\")\n",
    "                else:\n",
    "                    print(\"  인식된 개체명 없음\")\n",
    "\n",
    "                print(\"\\n📊 토큰별 NER 확률:\")\n",
    "                if result.get('ner_tokens') and result.get('ner_token_probabilities'):\n",
    "                    if ner_id2label:\n",
    "                        for token, probs in zip(result['ner_tokens'], result['ner_token_probabilities']):\n",
    "                            print(f\"  - 토큰: '{token}'\")\n",
    "                            sorted_probs = sorted(enumerate(probs), key=lambda item: item[1], reverse=True)\n",
    "                            # 상위 3개만 표시\n",
    "                            for tag_id, prob in sorted_probs[:3]:\n",
    "                                print(f\"      {ner_id2label.get(tag_id, f'ID_{tag_id}')}: {prob:.4f}\")\n",
    "                    else:\n",
    "                        print(\"  (오류: NER 레이블 매핑 정보(ner_id2label)를 찾을 수 없습니다.)\")\n",
    "                else:\n",
    "                    print(\"  토큰별 확률 정보 없음.\")\n",
    "\n",
    "                print(\"\\n📝 문자별 NER 태그 시각화:\")\n",
    "                print(f\"  원문: {result['text']}\")\n",
    "\n",
    "                text = result['text']\n",
    "                tag_markers = ['_'] * len(text)\n",
    "                processed_indices = set()\n",
    "\n",
    "                for entity in sorted(result.get('entities', []), key=lambda x: len(x['entity']), reverse=True):\n",
    "                    entity_text = entity['entity']\n",
    "                    entity_type = entity['label']\n",
    "                    start_pos = -1\n",
    "                    search_start = 0\n",
    "\n",
    "                    while True:\n",
    "                        temp_pos = text.find(entity_text, search_start)\n",
    "                        if temp_pos == -1: break\n",
    "\n",
    "                        is_overlapping = False\n",
    "                        for i in range(temp_pos, temp_pos + len(entity_text)):\n",
    "                            if i in processed_indices:\n",
    "                                is_overlapping = True\n",
    "                                break\n",
    "\n",
    "                        if not is_overlapping:\n",
    "                            start_pos = temp_pos\n",
    "                            break\n",
    "\n",
    "                        search_start = temp_pos + 1\n",
    "\n",
    "                    if start_pos != -1:\n",
    "                        end_pos = start_pos + len(entity_text)\n",
    "                        for i in range(start_pos, end_pos):\n",
    "                            if i not in processed_indices:\n",
    "                                tag_markers[i] = '^'\n",
    "                                processed_indices.add(i)\n",
    "\n",
    "                        tag_label = f\"[{entity_type}]\"\n",
    "                        for i, char in enumerate(tag_label):\n",
    "                            pos = start_pos + i\n",
    "                            if pos < len(tag_markers) and tag_markers[pos] == '^':\n",
    "                                tag_markers[pos] = char\n",
    "\n",
    "                print(f\"  태그: {''.join(tag_markers)}\")\n",
    "\n",
    "    # 이벤트 연결\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # 엔터키로도 예측 실행\n",
    "    def on_enter(widget):\n",
    "        button.click()\n",
    "\n",
    "    text_widget.on_submit(on_enter)\n",
    "\n",
    "    # UI 표시\n",
    "    display(title)\n",
    "\n",
    "    model_status = widgets.HTML(value=\"<h3>모델 로드 중...</h3>\")\n",
    "    display(model_status)\n",
    "\n",
    "    # 모델 로드\n",
    "    intent_loaded = load_intent_model()\n",
    "    ner_loaded = load_ner_model()\n",
    "\n",
    "    if intent_loaded and ner_loaded:\n",
    "        model_status.value = \"<h3 style='color:green'>✓ 모델 로드 완료</h3>\"\n",
    "    else:\n",
    "        model_status.value = \"<h3 style='color:red'>✗ 모델 로드 실패</h3>\"\n",
    "\n",
    "    # 입력 UI 표시\n",
    "    display(widgets.HBox([text_widget, button]))\n",
    "    display(output)\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    create_ui()\n",
    "else:\n",
    "    # Jupyter에서 직접 실행하는 경우\n",
    "    create_ui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
