{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec0780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "도서 검색 NLU 모델 훈련 시작\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Intent 분류 모델 훈련 시작\n",
      "==================================================\n",
      "Intent 레이블 (40개) 사용: {'search_book_title': 0, 'search_book_author': 1, 'search_book_location': 2, 'check_book_availability': 3, 'get_bestseller': 4, 'get_new_releases': 5, 'request_recommendation_genre': 6, 'request_recommendation_mood': 7, 'request_recommendation_topic': 8, 'request_recommendation_similar': 9, 'request_recommendation_reader': 10, 'search_space_availability': 11, 'reserve_space': 12, 'get_space_info': 13, 'check_space_reservation': 14, 'cancel_space_reservation': 15, 'search_program': 16, 'apply_program': 17, 'get_program_info': 18, 'check_program_application': 19, 'cancel_program_application': 20, 'get_library_hours': 21, 'inquire_service': 22, 'manage_membership': 23, 'check_loan_status': 24, 'extend_loan': 25, 'reserve_book': 26, 'check_reservation_status': 27, 'cancel_book_reservation': 28, 'check_overdue_status': 29, 'report_lost_item': 30, 'greeting': 31, 'gratitude': 32, 'closing': 33, 'affirmative': 34, 'negative': 35, 'abuse': 36, 'clarification': 37, 'out_of_scope': 38, 'repeat': 39}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c3cb80d62742df8519a73f474e2d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aacfc4bfe23419585224daf6b4a0a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent 데이터 샘플: {'label': 6, 'input_ids': [0, 5892, 4393, 1556, 3922, 2776, 2315, 35, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='685' max='685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [685/685 20:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.997100</td>\n",
       "      <td>1.116667</td>\n",
       "      <td>0.740876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.559700</td>\n",
       "      <td>0.687446</td>\n",
       "      <td>0.835766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>0.631319</td>\n",
       "      <td>0.846715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.639437</td>\n",
       "      <td>0.850365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.546600</td>\n",
       "      <td>0.522740</td>\n",
       "      <td>0.872263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.509796</td>\n",
       "      <td>0.883212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent 모델 평가 결과: {'eval_loss': 0.5097959041595459, 'eval_accuracy': 0.8832116788321168, 'eval_runtime': 11.1782, 'eval_samples_per_second': 24.512, 'eval_steps_per_second': 3.131, 'epoch': 5.0}\n",
      "Intent 모델 훈련 완료\n",
      "Intent 모델 및 레이블 정보 저장 완료: ./models/intent\n",
      "\n",
      "==================================================\n",
      "NER 모델 훈련 시작 (CRF 레이어 적용)\n",
      "==================================================\n",
      "토크나이저 테스트 '인문학' -> ['인문학']\n",
      "찾은 엔터티 타입: ['account_action', 'author', 'call_number', 'category', 'date', 'difficulty', 'duration', 'equipment', 'event_type', 'fee', 'format', 'genre', 'isbn', 'library_info_type', 'location', 'lost_item', 'mood', 'num_people', 'publisher', 'service_type', 'target_audience', 'time', 'timeOfDay', 'title', 'topic']\n",
      "NER 레이블 (51개): ['O', 'B-account_action', 'I-account_action', 'B-author', 'I-author', 'B-call_number', 'I-call_number', 'B-category', 'I-category', 'B-date', 'I-date', 'B-difficulty', 'I-difficulty', 'B-duration', 'I-duration', 'B-equipment', 'I-equipment', 'B-event_type', 'I-event_type', 'B-fee', 'I-fee', 'B-format', 'I-format', 'B-genre', 'I-genre', 'B-isbn', 'I-isbn', 'B-library_info_type', 'I-library_info_type', 'B-location', 'I-location', 'B-lost_item', 'I-lost_item', 'B-mood', 'I-mood', 'B-num_people', 'I-num_people', 'B-publisher', 'I-publisher', 'B-service_type', 'I-service_type', 'B-target_audience', 'I-target_audience', 'B-time', 'I-time', 'B-timeOfDay', 'I-timeOfDay', 'B-title', 'I-title', 'B-topic', 'I-topic']\n",
      "\n",
      "--- NER 데이터 전처리 시작 ---\n",
      "\n",
      "[데이터 0] 텍스트: \"예수는 역사다라는 책 있나요?\"\n",
      "  정의된 엔티티: [(0, 7, 'title'), (10, 11, 'format')]\n",
      "  문제가 있는 엔티티: (10, 11, format)\n",
      "  텍스트 길이: 16\n",
      "  문자별 BIO 태그:\n",
      "    '예': B-title\n",
      "    '수': I-title\n",
      "    '는': I-title\n",
      "    ' ': I-title\n",
      "    '역': I-title\n",
      "    '사': I-title\n",
      "    '다': I-title\n",
      "    '라': O\n",
      "    '는': O\n",
      "    ' ': O\n",
      "    '책': B-format\n",
      "    ' ': O\n",
      "    '있': O\n",
      "    '나': O\n",
      "    '요': O\n",
      "    '?': O\n",
      "  토큰화 결과: ['[CLS]', '예수', '##는', '역사', '##다', '##라는', '책', '있', '##나', '##요', '?', '[SEP]']\n",
      "  오프셋 매핑: [(0, 0), (0, 2), (2, 3), (4, 6), (6, 7), (7, 9), (10, 11), (12, 13), (13, 14), (14, 15), (15, 16), (0, 0)]\n",
      "  최종 토큰 레이블: ['IGN', 'B-title', 'I-title', 'I-title', 'I-title', 'O', 'B-format', 'O', 'O', 'O', 'O', 'IGN']\n",
      "\n",
      "[데이터 1] 텍스트: \"어머니 나무를 찾아서 책 찾아주세요.\"\n",
      "  정의된 엔티티: [(0, 11, 'title'), (12, 13, 'format')]\n",
      "  문제가 있는 엔티티: (12, 13, format)\n",
      "  텍스트 길이: 20\n",
      "  문자별 BIO 태그:\n",
      "    '어': B-title\n",
      "    '머': I-title\n",
      "    '니': I-title\n",
      "    ' ': I-title\n",
      "    '나': I-title\n",
      "    '무': I-title\n",
      "    '를': I-title\n",
      "    ' ': I-title\n",
      "    '찾': I-title\n",
      "    '아': I-title\n",
      "    '서': I-title\n",
      "    ' ': O\n",
      "    '책': B-format\n",
      "    ' ': O\n",
      "    '찾': O\n",
      "    '아': O\n",
      "    '주': O\n",
      "    '세': O\n",
      "    '요': O\n",
      "    '.': O\n",
      "  토큰화 결과: ['[CLS]', '어머니', '나무', '##를', '찾아', '##서', '책', '찾아', '##주', '##세요', '.', '[SEP]']\n",
      "  오프셋 매핑: [(0, 0), (0, 3), (4, 6), (6, 7), (8, 10), (10, 11), (12, 13), (14, 16), (16, 17), (17, 19), (19, 20), (0, 0)]\n",
      "  최종 토큰 레이블: ['IGN', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'B-format', 'O', 'O', 'O', 'O', 'IGN']\n",
      "\n",
      "[데이터 2] 텍스트: \"이처럼 사소한 것들 소장하고 있는지 궁금합니다.\"\n",
      "  정의된 엔티티: [(0, 10, 'title')]\n",
      "  문제가 있는 엔티티: (0, 10, title)\n",
      "  텍스트 길이: 26\n",
      "  문자별 BIO 태그:\n",
      "    '이': B-title\n",
      "    '처': I-title\n",
      "    '럼': I-title\n",
      "    ' ': I-title\n",
      "    '사': I-title\n",
      "    '소': I-title\n",
      "    '한': I-title\n",
      "    ' ': I-title\n",
      "    '것': I-title\n",
      "    '들': I-title\n",
      "    ' ': O\n",
      "    '소': O\n",
      "    '장': O\n",
      "    '하': O\n",
      "    '고': O\n",
      "    ' ': O\n",
      "    '있': O\n",
      "    '는': O\n",
      "    '지': O\n",
      "    ' ': O\n",
      "    '궁': O\n",
      "    '금': O\n",
      "    '합': O\n",
      "    '니': O\n",
      "    '다': O\n",
      "    '.': O\n",
      "  토큰화 결과: ['[CLS]', '이', '##처럼', '사소', '##한', '것', '##들', '소장', '##하고', '있', '##는지', '궁금', '##합니다', '.', '[SEP]']\n",
      "  오프셋 매핑: [(0, 0), (0, 1), (1, 3), (4, 6), (6, 7), (8, 9), (9, 10), (11, 13), (13, 15), (16, 17), (17, 19), (20, 22), (22, 25), (25, 26), (0, 0)]\n",
      "  최종 토큰 레이블: ['IGN', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IGN']\n",
      "NER 훈련 데이터 크기: 1138\n",
      "NER 평가 데이터 크기: 285\n",
      "NER 데이터 샘플: {'text': '미셸 케이건 작가의 알프스의 소녀 하이디 책은 꼭 읽어봐야 할 필독서지.', 'input_ids': [0, 17595, 5157, 2332, 4197, 2079, 20922, 2079, 5736, 4899, 2220, 1644, 2073, 677, 1508, 2051, 9766, 1892, 1885, 2405, 2112, 2118, 18, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 3, 4, 4, 0, 0, 47, 48, 48, 48, 48, 21, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='715' max='715' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [715/715 22:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Account Action</th>\n",
       "      <th>F1 Author</th>\n",
       "      <th>F1 Call Number</th>\n",
       "      <th>F1 Category</th>\n",
       "      <th>F1 Date</th>\n",
       "      <th>F1 Difficulty</th>\n",
       "      <th>F1 Duration</th>\n",
       "      <th>F1 Equipment</th>\n",
       "      <th>F1 Event Type</th>\n",
       "      <th>F1 Fee</th>\n",
       "      <th>F1 Format</th>\n",
       "      <th>F1 Genre</th>\n",
       "      <th>F1 Isbn</th>\n",
       "      <th>F1 Library Info Type</th>\n",
       "      <th>F1 Location</th>\n",
       "      <th>F1 Lost Item</th>\n",
       "      <th>F1 Mood</th>\n",
       "      <th>F1 Num People</th>\n",
       "      <th>F1 Publisher</th>\n",
       "      <th>F1 Service Type</th>\n",
       "      <th>F1 Target Audience</th>\n",
       "      <th>F1 Time</th>\n",
       "      <th>F1 Timeofday</th>\n",
       "      <th>F1 Title</th>\n",
       "      <th>F1 Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>41.172900</td>\n",
       "      <td>30.832035</td>\n",
       "      <td>0.812992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>19.918600</td>\n",
       "      <td>17.587696</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.608700</td>\n",
       "      <td>14.083743</td>\n",
       "      <td>0.926070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>28.110600</td>\n",
       "      <td>16.334330</td>\n",
       "      <td>0.927114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.726700</td>\n",
       "      <td>17.663099</td>\n",
       "      <td>0.937622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.742100</td>\n",
       "      <td>16.651510</td>\n",
       "      <td>0.939689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.946600</td>\n",
       "      <td>16.814814</td>\n",
       "      <td>0.939689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.8505    0.8349    0.8426       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.8788    0.9355    0.9062       124\n",
      "            genre     0.0000    0.0000    0.0000        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     0.0000    0.0000    0.0000         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     0.0000    0.0000    0.0000         1\n",
      "            title     0.7715    0.8443    0.8063       244\n",
      "\n",
      "        micro avg     0.8162    0.8098    0.8130       510\n",
      "        macro avg     0.2501    0.2615    0.2555       510\n",
      "     weighted avg     0.7646    0.8098    0.7862       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'B-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'I-format', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'B-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.9279    0.9450    0.9364       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.9535    0.9919    0.9723       124\n",
      "            genre     0.7333    0.9167    0.8148        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     0.0000    0.0000    0.0000         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     0.0000    0.0000    0.0000         1\n",
      "            title     0.8450    0.8934    0.8685       244\n",
      "\n",
      "        micro avg     0.8826    0.9137    0.8979       510\n",
      "        macro avg     0.3460    0.3747    0.3592       510\n",
      "     weighted avg     0.8689    0.9137    0.8904       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.9369    0.9541    0.9455       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.9538    1.0000    0.9764       124\n",
      "            genre     0.8462    0.9167    0.8800        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     1.0000    1.0000    1.0000         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     0.0000    0.0000    0.0000         1\n",
      "            title     0.9000    0.9221    0.9109       244\n",
      "\n",
      "        micro avg     0.9189    0.9333    0.9261       510\n",
      "        macro avg     0.4637    0.4793    0.4713       510\n",
      "     weighted avg     0.9045    0.9333    0.9187       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.9211    0.9633    0.9417       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.9466    1.0000    0.9725       124\n",
      "            genre     0.8214    0.9583    0.8846        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     1.0000    1.0000    1.0000         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     0.0000    0.0000    0.0000         1\n",
      "            title     0.9143    0.9180    0.9162       244\n",
      "\n",
      "        micro avg     0.9191    0.9353    0.9271       510\n",
      "        macro avg     0.4603    0.4840    0.4715       510\n",
      "     weighted avg     0.9050    0.9353    0.9196       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.9550    0.9725    0.9636       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.9609    0.9919    0.9762       124\n",
      "            genre     0.8462    0.9167    0.8800        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     1.0000    1.0000    1.0000         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     1.0000    1.0000    1.0000         1\n",
      "            title     0.9157    0.9344    0.9249       244\n",
      "\n",
      "        micro avg     0.9322    0.9431    0.9376       510\n",
      "        macro avg     0.5678    0.5816    0.5745       510\n",
      "     weighted avg     0.9196    0.9431    0.9312       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.9550    0.9725    0.9636       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.9685    0.9919    0.9801       124\n",
      "            genre     0.8400    0.8750    0.8571        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     0.3333    1.0000    0.5000         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     1.0000    1.0000    1.0000         1\n",
      "            title     0.9203    0.9467    0.9333       244\n",
      "\n",
      "        micro avg     0.9324    0.9471    0.9397       510\n",
      "        macro avg     0.5017    0.5786    0.5234       510\n",
      "     weighted avg     0.9220    0.9471    0.9341       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.9550    0.9725    0.9636       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.9685    0.9919    0.9801       124\n",
      "            genre     0.8462    0.9167    0.8800        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     0.5000    1.0000    0.6667         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     1.0000    1.0000    1.0000         1\n",
      "            title     0.9163    0.9426    0.9293       244\n",
      "\n",
      "        micro avg     0.9324    0.9471    0.9397       510\n",
      "        macro avg     0.5186    0.5824    0.5420       510\n",
      "     weighted avg     0.9207    0.9471    0.9335       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           author     0.9550    0.9725    0.9636       109\n",
      "             date     0.0000    0.0000    0.0000         1\n",
      "       event_type     0.0000    0.0000    0.0000         3\n",
      "           format     0.9685    0.9919    0.9801       124\n",
      "            genre     0.8400    0.8750    0.8571        24\n",
      "library_info_type     0.0000    0.0000    0.0000         1\n",
      "         location     0.3333    1.0000    0.5000         1\n",
      "             mood     0.0000    0.0000    0.0000         2\n",
      "  target_audience     1.0000    1.0000    1.0000         1\n",
      "            title     0.9203    0.9467    0.9333       244\n",
      "\n",
      "        micro avg     0.9324    0.9471    0.9397       510\n",
      "        macro avg     0.5017    0.5786    0.5234       510\n",
      "     weighted avg     0.9220    0.9471    0.9341       510\n",
      "\n",
      "\n",
      "예측 예시:\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'O', 'O', 'B-format', 'I-format', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['O', 'O', 'O', 'B-format', 'O', 'O', 'B-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "실제: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "예측: ['B-author', 'I-author', 'I-author', 'I-author', 'I-author', 'I-author', 'O', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'I-title', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "---\n",
      "NER 모델 평가 결과: {'eval_loss': 16.65151023864746, 'eval_f1': 0.9396887159533074, 'eval_f1_account_action': 1.0, 'eval_f1_author': 1.0, 'eval_f1_call_number': 1.0, 'eval_f1_category': 1.0, 'eval_f1_date': 1.0, 'eval_f1_difficulty': 1.0, 'eval_f1_duration': 1.0, 'eval_f1_equipment': 1.0, 'eval_f1_event_type': 1.0, 'eval_f1_fee': 1.0, 'eval_f1_format': 1.0, 'eval_f1_genre': 1.0, 'eval_f1_isbn': 1.0, 'eval_f1_library_info_type': 1.0, 'eval_f1_location': 1.0, 'eval_f1_lost_item': 1.0, 'eval_f1_mood': 1.0, 'eval_f1_num_people': 1.0, 'eval_f1_publisher': 1.0, 'eval_f1_service_type': 1.0, 'eval_f1_target_audience': 1.0, 'eval_f1_time': 1.0, 'eval_f1_timeOfDay': 1.0, 'eval_f1_title': 1.0, 'eval_f1_topic': 1.0, 'eval_runtime': 7.8284, 'eval_samples_per_second': 36.406, 'eval_steps_per_second': 4.599, 'epoch': 5.0}\n",
      "NER 모델 훈련 완료\n",
      "NER 모델 및 레이블 정보 저장 완료: ./models/ner\n",
      "\n",
      "==================================================\n",
      "도서 검색 NLU 모델 훈련 완료\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import commentjson\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    PreTrainedModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchcrf import CRF\n",
    "\n",
    "# --- 0. 기본 설정 ---\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "MAX_LEN = 128\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# 모델 저장 경로 설정\n",
    "INTENT_MODEL_DIR = \"./models/intent\"\n",
    "NER_MODEL_DIR = \"./models/ner\"\n",
    "INTENT_LABEL_PATH = os.path.join(INTENT_MODEL_DIR, \"intent_labels.jsonc\")\n",
    "NER_LABEL_PATH = os.path.join(NER_MODEL_DIR, \"ner_labels.jsonc\")\n",
    "\n",
    "# --- 1. 데이터 로드 ---\n",
    "# 1.1 Intent 분류를 위한 데이터\n",
    "def load_intent_data():\n",
    "    with open('intent_label_list.jsonc', 'r', encoding='utf-8') as f:\n",
    "        intent_label_list = commentjson.load(f)\n",
    "\n",
    "    intent_label_to_id = {label: i for i, label in enumerate(intent_label_list)}\n",
    "    intent_id_to_label = {i: label for label, i in intent_label_to_id.items()}\n",
    "\n",
    "    with open('intent_data.jsonc', 'r', encoding='utf-8') as f:\n",
    "        intent_data = commentjson.load(f)\n",
    "\n",
    "    return intent_data, intent_label_list, intent_label_to_id, intent_id_to_label\n",
    "\n",
    "# 1.2 NER을 위한 데이터\n",
    "def load_ner_data():\n",
    "    with open('ner_data.jsonc', 'r', encoding='utf-8') as f:\n",
    "        loaded_ner_data = commentjson.load(f)\n",
    "\n",
    "    ner_data = []\n",
    "    for item in loaded_ner_data:\n",
    "        entities_as_tuples = [tuple(entity_list) for entity_list in item.get(\"entities\", [])]\n",
    "        ner_data.append({\"text\": item.get(\"text\", \"\"), \"entities\": entities_as_tuples})\n",
    "\n",
    "    return ner_data\n",
    "\n",
    "# --- 2. Intent 분류 모델 훈련 ---\n",
    "def train_intent_model():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Intent 분류 모델 훈련 시작\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    intent_data, intent_label_list, intent_label_to_id, intent_id_to_label = load_intent_data()\n",
    "\n",
    "    num_intent_labels = len(intent_label_list)\n",
    "    print(f\"Intent 레이블 ({num_intent_labels}개) 사용: {intent_label_to_id}\")\n",
    "\n",
    "    intent_features = Features({\n",
    "        'text': Value('string'),\n",
    "        'label': ClassLabel(num_classes=num_intent_labels, names=intent_label_list)\n",
    "    })\n",
    "\n",
    "    intent_dataset = Dataset.from_list(intent_data, features=intent_features)\n",
    "\n",
    "    # 학습/평가 데이터 분리\n",
    "    train_test_datasets = intent_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test_datasets[\"train\"]\n",
    "    eval_dataset = train_test_datasets[\"test\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    def preprocess_intent_data(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"]\n",
    "        }\n",
    "\n",
    "    tokenized_train_dataset = train_dataset.map(\n",
    "        preprocess_intent_data,\n",
    "        batched=True,\n",
    "        remove_columns=['text']\n",
    "    )\n",
    "\n",
    "    tokenized_eval_dataset = eval_dataset.map(\n",
    "        preprocess_intent_data,\n",
    "        batched=True,\n",
    "        remove_columns=['text']\n",
    "    )\n",
    "\n",
    "    print(f\"Intent 데이터 샘플: {tokenized_train_dataset[0]}\")\n",
    "\n",
    "    intent_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    intent_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_intent_labels,\n",
    "        id2label=intent_id_to_label,\n",
    "        label2id=intent_label_to_id\n",
    "    )\n",
    "\n",
    "    def compute_intent_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results/intent\",\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        logging_dir='./logs/intent',\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        eval_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        learning_rate=5e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "    )\n",
    "\n",
    "    intent_trainer = Trainer(\n",
    "        model=intent_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=intent_data_collator,\n",
    "        compute_metrics=compute_intent_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    intent_trainer.train()\n",
    "    eval_result = intent_trainer.evaluate()\n",
    "    print(f\"Intent 모델 평가 결과: {eval_result}\")\n",
    "    print(\"Intent 모델 훈련 완료\")\n",
    "\n",
    "    # 모델 및 토크나이저 저장\n",
    "    os.makedirs(INTENT_MODEL_DIR, exist_ok=True)\n",
    "    intent_model.save_pretrained(INTENT_MODEL_DIR, safe_serialization=False)\n",
    "    tokenizer.save_pretrained(INTENT_MODEL_DIR)\n",
    "\n",
    "    # 레이블 정보 저장\n",
    "    with open(INTENT_LABEL_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"id2label\": intent_id_to_label,\n",
    "            \"label2id\": intent_label_to_id\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Intent 모델 및 레이블 정보 저장 완료: {INTENT_MODEL_DIR}\")\n",
    "\n",
    "# --- CRF를 적용한 커스텀 NER 모델 정의 ---\n",
    "class RobertaForTokenClassificationWithCRF(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        # Load RoBERTa model\n",
    "        self.roberta = AutoModelForTokenClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            config=config\n",
    "        ).roberta\n",
    "\n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # CRF layer\n",
    "        # self.crf = CRF(config.num_labels, batch_first=True)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # CRF loss calculation\n",
    "            # Convert -100 to a valid label index for CRF (e.g., 0)\n",
    "            mask = attention_mask.bool()\n",
    "\n",
    "            # Create label mask (True where label != -100)\n",
    "            label_mask = labels != -100\n",
    "\n",
    "            # Convert labels where they are -100 to 0 (temporary)\n",
    "            crf_labels = labels.clone()\n",
    "            crf_labels[~label_mask] = 0\n",
    "\n",
    "            # Calculate CRF loss using only valid positions\n",
    "            loss = -self.crf(logits, crf_labels, mask=mask).mean()\n",
    "\n",
    "        # During inference, get the most likely tag sequence\n",
    "        if labels is None:\n",
    "            # Use CRF to decode the most likely tag sequence\n",
    "            if attention_mask is not None:\n",
    "                mask = attention_mask.bool()\n",
    "                best_tags_list = self.crf.decode(logits, mask=mask)\n",
    "\n",
    "                # Convert list of lists to tensor\n",
    "                best_tags = torch.zeros_like(input_ids)\n",
    "                for i, tags in enumerate(best_tags_list):\n",
    "                    best_tags[i, :len(tags)] = torch.tensor(tags, device=best_tags.device)\n",
    "\n",
    "                return (best_tags, logits)\n",
    "            else:\n",
    "                best_tags_list = self.crf.decode(logits)\n",
    "                best_tags = torch.tensor(best_tags_list, device=logits.device)\n",
    "                return (best_tags, logits)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# --- 3. NER 모델 훈련 (CRF 포함) ---\n",
    "def train_ner_model():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NER 모델 훈련 시작 (CRF 레이어 적용)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    ner_data = load_ner_data()\n",
    "\n",
    "    # 3.1 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # 디버깅: 토크나이저 테스트\n",
    "    test_text = \"인문학\"\n",
    "    test_tokens = tokenizer.tokenize(test_text)\n",
    "    print(f\"토크나이저 테스트 '{test_text}' -> {test_tokens}\")\n",
    "\n",
    "    # 3.2 NER 레이블 정의 (BIO 형식)\n",
    "    # 먼저 모든 엔터티 타입 추출\n",
    "    entity_types = sorted(list(set(label for item in ner_data for _, _, label in item[\"entities\"])))\n",
    "    print(f\"찾은 엔터티 타입: {entity_types}\")\n",
    "\n",
    "    ner_labels = [\"O\"]  # Outside tag\n",
    "    for entity_type in entity_types:\n",
    "        ner_labels.extend([f\"B-{entity_type}\", f\"I-{entity_type}\"])\n",
    "\n",
    "    ner_label2id = {label: i for i, label in enumerate(ner_labels)}\n",
    "    ner_id2label = {i: label for label, i in ner_label2id.items()}\n",
    "\n",
    "    print(f\"NER 레이블 ({len(ner_labels)}개): {ner_labels}\")\n",
    "\n",
    "    # 3.3 NER 데이터 전처리 - 문자 단위 접근\n",
    "    preprocessed_ner_data = []\n",
    "\n",
    "    print(\"\\n--- NER 데이터 전처리 시작 ---\")\n",
    "\n",
    "    for example_idx, example in enumerate(ner_data):\n",
    "        text = example[\"text\"]\n",
    "        entities = example[\"entities\"]\n",
    "\n",
    "        if example_idx < 3:  # 처음 몇 개 예시만 상세 출력\n",
    "            print(f\"\\n[데이터 {example_idx}] 텍스트: \\\"{text}\\\"\")\n",
    "            print(f\"  정의된 엔티티: {entities}\")\n",
    "\n",
    "        # 1. 문자 단위 BIO 태깅 초기화\n",
    "        char_labels = [\"O\"] * len(text)\n",
    "\n",
    "        # 2. 엔티티에 따라 BIO 태그 할당\n",
    "        for start_char, end_char, entity_type in entities:\n",
    "            # 범위 체크 및 수정\n",
    "            if start_char < 0:\n",
    "                start_char = 0\n",
    "            if end_char > len(text):\n",
    "                end_char = len(text)\n",
    "\n",
    "            if start_char < end_char and start_char < len(text):\n",
    "                for i in range(start_char, end_char):\n",
    "                    if i == start_char:\n",
    "                        char_labels[i] = f\"B-{entity_type}\"\n",
    "                    else:\n",
    "                        char_labels[i] = f\"I-{entity_type}\"\n",
    "\n",
    "        if example_idx < 3 or start_char >= len(text) or end_char > len(text) or start_char < 0:\n",
    "            print(f\"  문제가 있는 엔티티: ({start_char}, {end_char}, {entity_type})\")\n",
    "            print(f\"  텍스트 길이: {len(text)}\")\n",
    "\n",
    "        if example_idx < 3:  # 상세 디버깅 출력\n",
    "            print(f\"  문자별 BIO 태그:\")\n",
    "            for i, (char, label) in enumerate(zip(text, char_labels)):\n",
    "                print(f\"    '{char}': {label}\")\n",
    "\n",
    "        # 3. 토큰화 및 토큰-문자 정렬\n",
    "        tokenized = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized['input_ids'])\n",
    "        offset_mapping = tokenized['offset_mapping']\n",
    "\n",
    "        if example_idx < 3:  # 상세 디버깅 출력\n",
    "            print(f\"  토큰화 결과: {tokens}\")\n",
    "            print(f\"  오프셋 매핑: {offset_mapping}\")\n",
    "\n",
    "        # 4. 토큰별 레이블 할당\n",
    "        token_labels = []\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            # 특수 토큰 처리\n",
    "            if start == end:\n",
    "                token_label = -100  # ignore_index\n",
    "            else:\n",
    "                # 토큰 시작 위치의 문자 레이블 사용\n",
    "                char_label = char_labels[start]\n",
    "                token_label = ner_label2id[char_label]\n",
    "\n",
    "                # 서브워드 토큰은 I- 태그로 변환 (WordPiece ##로 시작하는 경우)\n",
    "                if i > 0 and tokens[i].startswith(\"##\"):\n",
    "                    prev_label = token_labels[-1]\n",
    "                    if prev_label != -100 and ner_id2label[prev_label].startswith(\"B-\"):\n",
    "                        # B- -> I- 변환\n",
    "                        entity_type = ner_id2label[prev_label][2:]  # \"B-genre\" -> \"genre\"\n",
    "                        token_label = ner_label2id[f\"I-{entity_type}\"]\n",
    "\n",
    "            token_labels.append(token_label)\n",
    "\n",
    "        if example_idx < 3:  # 상세 디버깅 출력\n",
    "            print(f\"  최종 토큰 레이블: {[ner_id2label.get(l, 'IGN') for l in token_labels]}\")\n",
    "\n",
    "        # 5. 레이블 ID로 변환\n",
    "        preprocessed_ner_data.append({\n",
    "            \"text\": text,\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"labels\": token_labels\n",
    "        })\n",
    "\n",
    "    # 3.4 데이터셋 생성\n",
    "    ner_features = Features({\n",
    "        'text': Value('string'),\n",
    "        'input_ids': Sequence(Value('int32')),\n",
    "        'attention_mask': Sequence(Value('int32')),\n",
    "        'labels': Sequence(Value('int32'))\n",
    "    })\n",
    "\n",
    "    ner_dataset = Dataset.from_list(preprocessed_ner_data, features=ner_features)\n",
    "\n",
    "    # 학습/평가 데이터셋 분리\n",
    "    train_test_datasets = ner_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test_datasets[\"train\"]\n",
    "    eval_dataset = train_test_datasets[\"test\"]\n",
    "\n",
    "    print(f\"NER 훈련 데이터 크기: {len(train_dataset)}\")\n",
    "    print(f\"NER 평가 데이터 크기: {len(eval_dataset)}\")\n",
    "    print(f\"NER 데이터 샘플: {train_dataset[0]}\")\n",
    "\n",
    "    # 3.5 데이터 콜레이터 설정\n",
    "    ner_data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_multiple_of=8\n",
    "    )\n",
    "\n",
    "    # 3.6 모델 configuration 생성 및 CRF 모델 초기화\n",
    "    config = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=ner_id2label,\n",
    "        label2id=ner_label2id\n",
    "    ).config\n",
    "\n",
    "    # CRF 모델 초기화\n",
    "    ner_model = RobertaForTokenClassificationWithCRF(config)\n",
    "\n",
    "    # 3.7 평가 지표 계산 함수 - CRF를 위해 수정\n",
    "    def compute_ner_metrics(p):\n",
    "        predictions, labels = p\n",
    "\n",
    "        # predictions는 이미 CRF를 통과한 최적 경로\n",
    "        # predictions, _ = predictions # 첫 번째 요소만 사용\n",
    "\n",
    "        # 실제 토큰의 예측값과 레이블만 추출\n",
    "        true_predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        for prediction, label in zip(predictions, labels):\n",
    "            true_pred = []\n",
    "            true_label = []\n",
    "\n",
    "            for p, l in zip(prediction, label):\n",
    "                if l != -100:  # -100은 무시\n",
    "                    true_pred.append(ner_id2label[p.item()])\n",
    "                    true_label.append(ner_id2label[l.item()])\n",
    "\n",
    "            true_predictions.append(true_pred)\n",
    "            true_labels.append(true_label)\n",
    "\n",
    "        # seqeval의 f1_score 계산\n",
    "        try:\n",
    "            f1 = f1_score(true_labels, true_predictions)\n",
    "            report = classification_report(true_labels, true_predictions, digits=4)\n",
    "            print(\"\\nNER Classification Report:\\n\", report)\n",
    "\n",
    "            # 세부 클래스별 결과 분석\n",
    "            class_results = {}\n",
    "            for label in ner_labels:\n",
    "                if label != \"O\" and label.startswith(\"B-\"):\n",
    "                    entity_type = label[2:]  # \"B-genre\" -> \"genre\"\n",
    "                    label_f1 = f1_score([[label]], [[label]], average='macro')\n",
    "                    class_results[entity_type] = label_f1\n",
    "\n",
    "            # 상세 예측 예시 출력\n",
    "            print(\"\\n예측 예시:\")\n",
    "            for i in range(min(5, len(true_labels))):\n",
    "                print(f\"실제: {true_labels[i]}\")\n",
    "                print(f\"예측: {true_predictions[i]}\")\n",
    "                print(\"---\")\n",
    "\n",
    "            return {\n",
    "                \"f1\": f1,\n",
    "                **{f\"f1_{key}\": val for key, val in class_results.items()}\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating NER metrics: {e}\")\n",
    "            return {\"f1\": 0.0}\n",
    "\n",
    "    # 3.8 커스텀 Trainer 클래스 정의 (CRF 모델 지원)\n",
    "    class CRFTrainer(Trainer):\n",
    "        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "            has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if has_labels:\n",
    "                    with self.compute_loss_context_manager():\n",
    "                        outputs = model(**inputs)\n",
    "                        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "                    labels = tuple(inputs.get(name) for name in self.label_names)\n",
    "\n",
    "                    # CRF 모델에서는 logits이 예측값이 아니라 방출 점수\n",
    "                    # 실제 예측은 CRF decode를 통해 수행\n",
    "                    with torch.no_grad():\n",
    "                        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[1]\n",
    "                        mask = inputs[\"attention_mask\"].bool()\n",
    "                        predictions = model.crf.decode(logits, mask=mask)\n",
    "\n",
    "                        # predictions를 tensor로 변환\n",
    "                        max_len = inputs[\"input_ids\"].size(1)\n",
    "                        pred_tensor = torch.zeros_like(inputs[\"input_ids\"])\n",
    "                        for i, pred in enumerate(predictions):\n",
    "                            length = min(len(pred), max_len)\n",
    "                            pred_tensor[i, :length] = torch.tensor(pred[:length], device=pred_tensor.device)\n",
    "\n",
    "                        return (loss.detach(), pred_tensor, labels[0])\n",
    "                else:\n",
    "                    # 예측 모드\n",
    "                    outputs = model(**inputs)\n",
    "                    predictions, _ = outputs  # CRF decode 결과와 방출 점수\n",
    "                    return (None, predictions, None)\n",
    "\n",
    "    # 3.9 훈련 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results/ner_crf\",\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        logging_dir='./logs/ner_crf',\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        greater_is_better=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"none\",\n",
    "        learning_rate=5e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "    )\n",
    "\n",
    "    # 3.10 CRF Trainer 정의\n",
    "    ner_trainer = CRFTrainer(\n",
    "        model=ner_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=ner_data_collator,\n",
    "        compute_metrics=compute_ner_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # 3.11 모델 훈련\n",
    "    ner_trainer.train()\n",
    "    eval_result = ner_trainer.evaluate()\n",
    "    print(f\"NER 모델 평가 결과: {eval_result}\")\n",
    "    print(\"NER 모델 훈련 완료\")\n",
    "\n",
    "    # 3.12 모델 및 레이블 정보 저장\n",
    "    os.makedirs(NER_MODEL_DIR, exist_ok=True)\n",
    "    gc.collect()\n",
    "    ner_model.cpu()\n",
    "    torch.save(ner_model.state_dict(), os.path.join(NER_MODEL_DIR, \"pytorch_model.bin\"))\n",
    "\n",
    "    # Config 정보 저장\n",
    "    config_dict = config.to_dict()\n",
    "    config_dict[\"model_type\"] = \"roberta_crf\"  # 커스텀 모델 타입 표시\n",
    "    with open(os.path.join(NER_MODEL_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 토크나이저 저장\n",
    "    tokenizer.save_pretrained(NER_MODEL_DIR)\n",
    "\n",
    "    # 레이블 매핑 저장\n",
    "    with open(NER_LABEL_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"id2label\": ner_id2label,\n",
    "            \"label2id\": ner_label2id\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"NER 모델 및 레이블 정보 저장 완료: {NER_MODEL_DIR}\")\n",
    "\n",
    "# --- 메인 함수 ---\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"도서 검색 NLU 모델 훈련 시작\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Intent 모델 훈련\n",
    "    train_intent_model()\n",
    "\n",
    "    # NER 모델 훈련 (CRF 포함)\n",
    "    train_ner_model()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"도서 검색 NLU 모델 훈련 완료\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47eb931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\spring\\CaterpillarAi\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
