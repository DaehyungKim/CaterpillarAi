{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "개선된 통합 NLU 모델 훈련 시작 (Intent + NER)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "데이터 전처리 및 통합 시작\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인텐트 데이터 로드 중...\n",
      "인텐트 레이블 수: 40\n",
      "인텐트 데이터 수: 1371\n",
      "Intent 레이블 (40개): ['search_book_title', 'search_book_author', 'search_book_location', 'check_book_availability', 'get_bestseller', 'get_new_releases', 'request_recommendation_genre', 'request_recommendation_mood', 'request_recommendation_topic', 'request_recommendation_similar', 'request_recommendation_reader', 'search_space_availability', 'reserve_space', 'get_space_info', 'check_space_reservation', 'cancel_space_reservation', 'search_program', 'apply_program', 'get_program_info', 'check_program_application', 'cancel_program_application', 'get_library_hours', 'inquire_service', 'manage_membership', 'check_loan_status', 'extend_loan', 'reserve_book', 'check_reservation_status', 'cancel_book_reservation', 'check_overdue_status', 'report_lost_item', 'greeting', 'gratitude', 'closing', 'affirmative', 'negative', 'abuse', 'clarification', 'out_of_scope', 'repeat']\n",
      "NER 데이터 로드 중...\n",
      "NER 데이터 수: 1425\n",
      "NER 레이블 (51개): ['O', 'B-account_action', 'I-account_action', 'B-author', 'I-author', 'B-call_number', 'I-call_number', 'B-category', 'I-category', 'B-date', 'I-date', 'B-difficulty', 'I-difficulty', 'B-duration', 'I-duration', 'B-equipment', 'I-equipment', 'B-event_type', 'I-event_type', 'B-fee', 'I-fee', 'B-format', 'I-format', 'B-genre', 'I-genre', 'B-isbn', 'I-isbn', 'B-library_info_type', 'I-library_info_type', 'B-location', 'I-location', 'B-lost_item', 'I-lost_item', 'B-mood', 'I-mood', 'B-num_people', 'I-num_people', 'B-publisher', 'I-publisher', 'B-service_type', 'I-service_type', 'B-target_audience', 'I-target_audience', 'B-time', 'I-time', 'B-timeOfDay', 'I-timeOfDay', 'B-title', 'I-title', 'B-topic', 'I-topic']\n",
      "\n",
      "--- Intent 데이터 전처리 ---\n",
      "인텐트 클래스별 데이터 수:\n",
      "  - search_book_title: 186개\n",
      "  - None: 8개\n",
      "  - search_book_author: 116개\n",
      "  - search_book_location: 213개\n",
      "  - check_book_availability: 183개\n",
      "  - get_bestseller: 75개\n",
      "  - get_new_releases: 89개\n",
      "  - request_recommendation_genre: 295개\n",
      "  - request_recommendation_mood: 45개\n",
      "  - request_recommendation_topic: 5개\n",
      "  - request_recommendation_similar: 5개\n",
      "  - request_recommendation_reader: 5개\n",
      "  - search_space_availability: 5개\n",
      "  - reserve_space: 5개\n",
      "  - get_space_info: 5개\n",
      "  - check_space_reservation: 5개\n",
      "  - cancel_space_reservation: 5개\n",
      "  - search_program: 5개\n",
      "  - apply_program: 5개\n",
      "  - get_program_info: 5개\n",
      "  - check_program_application: 5개\n",
      "  - cancel_program_application: 5개\n",
      "  - get_library_hours: 5개\n",
      "  - inquire_service: 5개\n",
      "  - manage_membership: 5개\n",
      "  - check_loan_status: 5개\n",
      "  - extend_loan: 5개\n",
      "  - reserve_book: 5개\n",
      "  - check_reservation_status: 5개\n",
      "  - cancel_book_reservation: 5개\n",
      "  - check_overdue_status: 5개\n",
      "  - report_lost_item: 5개\n",
      "  - greeting: 5개\n",
      "  - gratitude: 5개\n",
      "  - closing: 5개\n",
      "  - affirmative: 5개\n",
      "  - negative: 5개\n",
      "  - abuse: 6개\n",
      "  - clarification: 5개\n",
      "  - out_of_scope: 5개\n",
      "  - repeat: 5개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "인텐트 데이터 처리:  69%|██████▉   | 949/1371 [00:00<00:00, 4914.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Intent 데이터 0] '나루토 책 있어?' -> 의도: search_book_title\n",
      "\n",
      "[Intent 데이터 1] '식물학자의 숲속 일기라는 책 있나요?' -> 의도: search_book_title\n",
      "\n",
      "[Intent 데이터 2] '사랑의 기술 책 찾아주세요.' -> 의도: search_book_title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "인텐트 데이터 처리: 100%|██████████| 1371/1371 [00:00<00:00, 4992.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NER 데이터 전처리 ---\n",
      "엔티티 유형별 개수:\n",
      "  - title: 1171개\n",
      "  - format: 700개\n",
      "  - author: 529개\n",
      "  - genre: 90개\n",
      "  - isbn: 2개\n",
      "  - call_number: 2개\n",
      "  - topic: 5개\n",
      "  - category: 5개\n",
      "  - mood: 3개\n",
      "  - target_audience: 4개\n",
      "  - date: 6개\n",
      "  - timeOfDay: 2개\n",
      "  - location: 12개\n",
      "  - time: 1개\n",
      "  - event_type: 8개\n",
      "  - library_info_type: 2개\n",
      "  - service_type: 2개\n",
      "  - lost_item: 3개\n",
      "  - account_action: 1개\n",
      "  - publisher: 1개\n",
      "  - difficulty: 1개\n",
      "  - duration: 1개\n",
      "  - num_people: 1개\n",
      "  - equipment: 1개\n",
      "  - fee: 1개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER 데이터 처리:  26%|██▋       | 375/1425 [00:00<00:00, 1920.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NER 데이터 0] '나루토 책 있어?' -> 엔티티: [(0, 3, 'title'), (4, 5, 'format')]\n",
      "  인텐트: search_program\n",
      "  토큰화: ['[CLS]', '나루', '##토', '책', '있', '##어', '?', '[SEP]', '[PAD]', '[PAD]']... (총 64개)\n",
      "  NER 레이블: ['IGN', 'B-title', 'I-title', 'B-format', 'O', 'O', 'O', 'IGN', 'IGN', 'IGN']... (총 64개)\n",
      "\n",
      "[NER 데이터 1] '예수는 역사다라는 책 있나요?' -> 엔티티: [(0, 7, 'title'), (10, 11, 'format')]\n",
      "  인텐트: affirmative\n",
      "  토큰화: ['[CLS]', '예수', '##는', '역사', '##다', '##라는', '책', '있', '##나', '##요']... (총 64개)\n",
      "  NER 레이블: ['IGN', 'B-title', 'I-title', 'I-title', 'I-title', 'O', 'B-format', 'O', 'O', 'O']... (총 64개)\n",
      "\n",
      "[NER 데이터 2] '어머니 나무를 찾아서 책 찾아주세요.' -> 엔티티: [(0, 11, 'title'), (12, 13, 'format')]\n",
      "  인텐트: check_loan_status\n",
      "  토큰화: ['[CLS]', '어머니', '나무', '##를', '찾아', '##서', '책', '찾아', '##주', '##세요']... (총 64개)\n",
      "  NER 레이블: ['IGN', 'B-title', 'I-title', 'I-title', 'I-title', 'I-title', 'B-format', 'O', 'O', 'O']... (총 64개)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER 데이터 처리: 100%|██████████| 1425/1425 [00:00<00:00, 2488.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "통합 데이터셋 크기: 3363\n",
      "훈련 데이터 크기: 500\n",
      "검증 데이터 크기: 100\n",
      "테스트 데이터 크기: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\model.safetensors\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading weights file model.safetensors from cache at C:\\Users\\JA104\\.cache\\huggingface\\hub\\models--klue--roberta-base\\snapshots\\02f94ba5e3fcb7e2a58a390b8639b0fac974a8da\\model.safetensors\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `ImprovedRobertaForJointIntentAndNER.forward` and have been ignored: intent_label, text. If intent_label, text are not expected by `ImprovedRobertaForJointIntentAndNER.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 114,235,882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 훈련 시작...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import commentjson\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import logging  # 파이썬 기본 로깅\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    PreTrainedModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "# transformers의 로깅을 별도 이름으로 임포트\n",
    "from transformers import logging as transformers_logging\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score as sklearn_f1_score\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from torchcrf import CRF\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# --- 0. 기본 설정 ---\n",
    "MODEL_NAME = \"klue/roberta-base\"  # 모델 크기 증가 (base -> large)\n",
    "MAX_LEN = 128  # 시퀀스 길이 증가\n",
    "EPOCHS = 3    # 에폭 수 증가\n",
    "BATCH_SIZE = 4  # 배치 크기 증가\n",
    "LEARNING_RATE = 5e-5  # 학습률 최적화\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "INTENT_WEIGHT = 0.4  # Intent 태스크 가중치 조정\n",
    "NER_WEIGHT = 0.6     # NER 태스크 가중치 조정 (더 어려운 태스크에 가중치 부여)\n",
    "\n",
    "def limit_dataset_size(dataset, max_size=1000):\n",
    "    \"\"\"데이터셋 크기를 제한하는 함수\"\"\"\n",
    "    if len(dataset) > max_size:\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:max_size]\n",
    "        return dataset.select(indices)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 모델 저장 경로 설정\n",
    "INTEGRATED_MODEL_DIR = \"./models/integrated_nlu_improved\"\n",
    "INTEGRATED_LABEL_PATH = os.path.join(INTEGRATED_MODEL_DIR, \"nlu_labels.jsonc\")\n",
    "\n",
    "# Seed 설정 (재현성)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 로깅 설정\n",
    "transformers_logging.set_verbosity_info()  # transformers 로깅 설정\n",
    "logger = logging.getLogger(__name__)  # 파이썬 기본 로깅 설정\n",
    "\n",
    "# --- 나머지 코드는 동일 ---\n",
    "\n",
    "# --- 1. 데이터 로드 함수 ---\n",
    "def load_intent_data():\n",
    "    print(\"인텐트 데이터 로드 중...\")\n",
    "    with open('intent_label_list.jsonc', 'r', encoding='utf-8') as f:\n",
    "        intent_label_list = commentjson.load(f)\n",
    "\n",
    "    intent_label_to_id = {label: i for i, label in enumerate(intent_label_list)}\n",
    "    intent_id_to_label = {i: label for i, label in enumerate(intent_label_list)}\n",
    "\n",
    "    with open('intent_data.jsonc', 'r', encoding='utf-8') as f:\n",
    "        intent_data = commentjson.load(f)\n",
    "\n",
    "    print(f\"인텐트 레이블 수: {len(intent_label_list)}\")\n",
    "    print(f\"인텐트 데이터 수: {len(intent_data)}\")\n",
    "    return intent_data, intent_label_list, intent_label_to_id, intent_id_to_label\n",
    "\n",
    "def load_ner_data():\n",
    "    print(\"NER 데이터 로드 중...\")\n",
    "    with open('ner_data.jsonc', 'r', encoding='utf-8') as f:\n",
    "        loaded_ner_data = commentjson.load(f)\n",
    "\n",
    "    ner_data = []\n",
    "    for item in loaded_ner_data:\n",
    "        entities_as_tuples = [tuple(entity_list) for entity_list in item.get(\"entities\", [])]\n",
    "        ner_data.append({\"text\": item.get(\"text\", \"\"), \"entities\": entities_as_tuples})\n",
    "\n",
    "    print(f\"NER 데이터 수: {len(ner_data)}\")\n",
    "    return ner_data\n",
    "\n",
    "# --- 2. 통합 NLU 모델 정의 (Intent + NER) - 개선 ---\n",
    "class ImprovedRobertaForJointIntentAndNER(PreTrainedModel):\n",
    "    def __init__(self, config, intent_label_to_id, ner_label_to_id):\n",
    "        super().__init__(config)\n",
    "        self.num_intent_labels = len(intent_label_to_id)\n",
    "        self.num_ner_labels = len(ner_label_to_id)\n",
    "\n",
    "        # 기본 모델 로드\n",
    "        self.roberta = AutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "        # Intent 분류를 위한 헤드 (개선: 더 복잡한 분류기)\n",
    "        self.intent_dropout = nn.Dropout(0.1)  # 드롭아웃 감소\n",
    "        self.ner_lstm = nn.LSTM(\n",
    "            input_size=config.hidden_size,\n",
    "            hidden_size=config.hidden_size // 2,  # 양방향이므로 절반 크기\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.intent_classifier = nn.Linear(config.hidden_size, self.num_intent_labels)  # 단순 선형 분류기\n",
    "\n",
    "        # NER을 위한 헤드 (개선: BiLSTM + CRF)\n",
    "        self.ner_dropout = nn.Dropout(0.1)  # 드롭아웃 감소\n",
    "        self.ner_classifier = nn.Linear(config.hidden_size, self.num_ner_labels)\n",
    "\n",
    "        # CRF 레이어 (NER용)\n",
    "        self.crf = CRF(self.num_ner_labels, batch_first=True)\n",
    "\n",
    "        # 손실 가중치 (두 작업의 균형을 위해 조정)\n",
    "        self.intent_loss_weight = INTENT_WEIGHT\n",
    "        self.ner_loss_weight = NER_WEIGHT\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        intent_labels=None,\n",
    "        ner_labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        # RoBERTa 인코더 실행\n",
    "        outputs = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]  # 모든 토큰의 임베딩\n",
    "        pooled_output = sequence_output[:, 0, :]  # [CLS] 토큰 임베딩 (Intent 용)\n",
    "\n",
    "        # Intent 분류 (개선된 분류기)\n",
    "        intent_output = self.intent_dropout(pooled_output)\n",
    "        intent_logits = self.intent_classifier(intent_output)\n",
    "\n",
    "        # NER 분류 (BiLSTM + CRF)\n",
    "        ner_output = self.ner_dropout(sequence_output)\n",
    "        ner_lstm_output, _ = self.ner_lstm(ner_output)\n",
    "        ner_logits = self.ner_classifier(ner_lstm_output)\n",
    "\n",
    "        # 손실 계산\n",
    "        total_loss = None\n",
    "        intent_loss = None\n",
    "        ner_loss = None\n",
    "\n",
    "        if intent_labels is not None:\n",
    "            # Intent 손실 계산 (Label Smoothing 추가)\n",
    "            intent_loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "            intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_labels.view(-1))\n",
    "\n",
    "        if ner_labels is not None:\n",
    "            # NER 손실 계산 (CRF)\n",
    "            mask = attention_mask.bool()\n",
    "\n",
    "            # -100을 유효한 레이블 인덱스로 변환 (CRF용)\n",
    "            label_mask = ner_labels != -100\n",
    "            crf_labels = ner_labels.clone()\n",
    "            crf_labels[~label_mask] = 0\n",
    "\n",
    "            # CRF 손실 계산\n",
    "            ner_loss = -self.crf(ner_logits, crf_labels, mask=mask).mean()\n",
    "\n",
    "        # 총 손실 계산 (두 작업의 가중치 합)\n",
    "        if intent_loss is not None and ner_loss is not None:\n",
    "            total_loss = self.intent_loss_weight * intent_loss + self.ner_loss_weight * ner_loss\n",
    "\n",
    "        # 예측 단계\n",
    "        intent_predictions = None\n",
    "        ner_predictions = None\n",
    "\n",
    "        if intent_labels is None:\n",
    "            intent_predictions = torch.argmax(intent_logits, dim=1)\n",
    "\n",
    "        if ner_labels is None:\n",
    "            if attention_mask is not None:\n",
    "                mask = attention_mask.bool()\n",
    "                best_ner_tags_list = self.crf.decode(ner_logits, mask=mask)\n",
    "\n",
    "                # 리스트를 텐서로 변환\n",
    "                ner_predictions = torch.zeros_like(input_ids)\n",
    "                for i, tags in enumerate(best_ner_tags_list):\n",
    "                    ner_predictions[i, :len(tags)] = torch.tensor(tags, device=ner_predictions.device)\n",
    "            else:\n",
    "                best_ner_tags_list = self.crf.decode(ner_logits)\n",
    "                ner_predictions = torch.tensor(best_ner_tags_list, device=ner_logits.device)\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"intent_loss\": intent_loss,\n",
    "            \"ner_loss\": ner_loss,\n",
    "            \"intent_logits\": intent_logits,\n",
    "            \"ner_logits\": ner_logits,\n",
    "            \"intent_predictions\": intent_predictions,\n",
    "            \"ner_predictions\": ner_predictions\n",
    "        }\n",
    "\n",
    "# --- 3. 데이터 전처리 및 통합 ---\n",
    "def preprocess_and_merge_data():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"데이터 전처리 및 통합 시작\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 3.1 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # 3.2 Intent 데이터 로드\n",
    "    intent_data, intent_label_list, intent_label_to_id, intent_id_to_label = load_intent_data()\n",
    "    print(f\"Intent 레이블 ({len(intent_label_list)}개): {intent_label_list}\")\n",
    "\n",
    "    # 3.3 NER 데이터 로드\n",
    "    ner_data = load_ner_data()\n",
    "\n",
    "    # 3.4 NER 레이블 정의 (BIO 형식)\n",
    "    entity_types = set()\n",
    "    for item in ner_data:\n",
    "        for _, _, label in item[\"entities\"]:\n",
    "            entity_types.add(label)\n",
    "    entity_types = sorted(list(entity_types))\n",
    "\n",
    "    ner_labels = [\"O\"]  # Outside tag\n",
    "    for entity_type in entity_types:\n",
    "        ner_labels.extend([f\"B-{entity_type}\", f\"I-{entity_type}\"])\n",
    "\n",
    "    ner_label_to_id = {label: i for i, label in enumerate(ner_labels)}\n",
    "    ner_id_to_label = {i: label for label, i in ner_label_to_id.items()}\n",
    "    print(f\"NER 레이블 ({len(ner_labels)}개): {ner_labels}\")\n",
    "\n",
    "    # 3.5 데이터 통합 및 증강\n",
    "    integrated_data = []\n",
    "\n",
    "    # Intent 데이터 전처리 및 추가\n",
    "    print(\"\\n--- Intent 데이터 전처리 ---\")\n",
    "\n",
    "    # 각 인텐트 클래스별 데이터 수 파악\n",
    "    intent_class_counts = {}\n",
    "    for item in intent_data:\n",
    "        intent_label = None\n",
    "        possible_keys = [\"intent_label\", \"intent\", \"label\"]\n",
    "\n",
    "        for key in possible_keys:\n",
    "            if key in item:\n",
    "                label_value = item[key]\n",
    "                if isinstance(label_value, str):\n",
    "                    intent_label = label_value\n",
    "                else:\n",
    "                    intent_label = intent_id_to_label.get(int(label_value), \"unknown\")\n",
    "                break\n",
    "\n",
    "        if intent_label not in intent_class_counts:\n",
    "            intent_class_counts[intent_label] = 0\n",
    "        intent_class_counts[intent_label] += 1\n",
    "\n",
    "    print(\"인텐트 클래스별 데이터 수:\")\n",
    "    for label, count in intent_class_counts.items():\n",
    "        print(f\"  - {label}: {count}개\")\n",
    "\n",
    "    # Intent 데이터 추가\n",
    "    for idx, item in enumerate(tqdm(intent_data, desc=\"인텐트 데이터 처리\")):\n",
    "        try:\n",
    "            # 텍스트 필드 확인\n",
    "            if \"text\" not in item:\n",
    "                continue\n",
    "\n",
    "            text = item[\"text\"]\n",
    "\n",
    "            # 레이블을 찾기 위한 다양한 키 시도\n",
    "            intent_label = None\n",
    "            possible_keys = [\"intent_label\", \"intent\", \"label\"]\n",
    "\n",
    "            for key in possible_keys:\n",
    "                if key in item:\n",
    "                    label_value = item[key]\n",
    "                    # 레이블이 문자열이면 ID로 변환, 이미 숫자면 그대로 사용\n",
    "                    if isinstance(label_value, str):\n",
    "                        if label_value in intent_label_to_id:\n",
    "                            intent_label = intent_label_to_id[label_value]\n",
    "                        elif label_value.isdigit() and int(label_value) < len(intent_label_list):\n",
    "                            intent_label = int(label_value)\n",
    "                    else:  # 숫자인 경우\n",
    "                        intent_label = int(label_value) if isinstance(label_value, (int, float)) else 0\n",
    "\n",
    "                    break\n",
    "\n",
    "            # 레이블을 찾지 못한 경우 기본값 사용\n",
    "            if intent_label is None:\n",
    "                intent_label = 0\n",
    "\n",
    "            # 토큰화\n",
    "            tokenized = tokenizer(\n",
    "                text,\n",
    "                return_offsets_mapping=True,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LEN,\n",
    "                padding='max_length'  # 패딩 추가\n",
    "            )\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offset_mapping = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # NER 레이블은 모두 무시 (-100)\n",
    "            ner_token_labels = [-100] * len(input_ids)\n",
    "\n",
    "            if idx < 3:  # 샘플 출력\n",
    "                print(f\"\\n[Intent 데이터 {idx}] '{text}' -> 의도: {intent_id_to_label.get(intent_label, '알 수 없음')}\")\n",
    "\n",
    "            integrated_data.append({\n",
    "                \"text\": text,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"intent_label\": intent_label,\n",
    "                \"ner_labels\": ner_token_labels\n",
    "            })\n",
    "\n",
    "            # 데이터 증강: 소량의 노이즈 추가 (랜덤 마스킹)\n",
    "            if idx % 3 == 0:  # 3개마다 1개씩 증강\n",
    "                noised_input_ids = input_ids.copy()\n",
    "                # 10%의 토큰을 랜덤하게 마스킹 또는 다른 토큰으로 대체\n",
    "                for i in range(1, len(noised_input_ids)-1):  # [CLS]와 [SEP] 토큰은 보존\n",
    "                    if random.random() < 0.1:  # 10% 확률로 마스킹\n",
    "                        noised_input_ids[i] = tokenizer.mask_token_id\n",
    "\n",
    "                integrated_data.append({\n",
    "                    \"text\": text + \" (augmented)\",\n",
    "                    \"input_ids\": noised_input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"intent_label\": intent_label,\n",
    "                    \"ner_labels\": ner_token_labels\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Intent 데이터 처리 중 오류 발생 (항목 {idx}): {e}\")\n",
    "            continue\n",
    "\n",
    "    # NER 데이터 전처리 및 추가\n",
    "    print(\"\\n--- NER 데이터 전처리 ---\")\n",
    "\n",
    "    # 엔티티 유형별 개수 파악\n",
    "    entity_type_counts = {}\n",
    "    for item in ner_data:\n",
    "        for _, _, entity_type in item[\"entities\"]:\n",
    "            if entity_type not in entity_type_counts:\n",
    "                entity_type_counts[entity_type] = 0\n",
    "            entity_type_counts[entity_type] += 1\n",
    "\n",
    "    print(\"엔티티 유형별 개수:\")\n",
    "    for entity_type, count in entity_type_counts.items():\n",
    "        print(f\"  - {entity_type}: {count}개\")\n",
    "\n",
    "    # Intent 간 균형을 위한 인텐트 분포 생성\n",
    "    intent_counts = {i: 0 for i in range(len(intent_label_list))}\n",
    "    for item in integrated_data:\n",
    "        intent_label = item.get(\"intent_label\")\n",
    "        if intent_label is not None:\n",
    "            intent_counts[intent_label] += 1\n",
    "\n",
    "    # 가장 많은 수의 인텐트 ID 찾기\n",
    "    max_intent_count = max(intent_counts.values())\n",
    "    min_intent_count = min(intent_counts.values())\n",
    "    intent_weights = {i: max_intent_count / (count + 1) for i, count in intent_counts.items()}\n",
    "\n",
    "    # NER 데이터에 인텐트 분포 적용\n",
    "    for idx, item in enumerate(tqdm(ner_data, desc=\"NER 데이터 처리\")):\n",
    "        text = item[\"text\"]\n",
    "        entities = item[\"entities\"]\n",
    "\n",
    "        # 적절한 인텐트 할당 (매핑 또는 무작위)\n",
    "        # 의도적으로 부족한 인텐트 클래스에 가중치 부여하여 할당\n",
    "        weighted_intent_ids = []\n",
    "        for i, weight in intent_weights.items():\n",
    "            weighted_intent_ids.extend([i] * int(weight * 10))\n",
    "\n",
    "        intent_label = random.choice(weighted_intent_ids)\n",
    "\n",
    "        # 텍스트 기반 인텐트 힌트 (텍스트에 특정 키워드가 있으면 관련 인텐트로 설정)\n",
    "        # 예: \"책, 도서, 대출\" -> 도서 관련 인텐트\n",
    "        # 이 부분은 프로젝트에 맞게 구체적으로 구현 필요\n",
    "\n",
    "        # 1. 문자 단위 BIO 태깅\n",
    "        char_labels = [\"O\"] * len(text)\n",
    "\n",
    "        # 2. 엔티티에 따라 BIO 태그 할당\n",
    "        for start_char, end_char, entity_type in entities:\n",
    "            if start_char < 0:\n",
    "                start_char = 0\n",
    "            if end_char > len(text):\n",
    "                end_char = len(text)\n",
    "\n",
    "            if start_char < end_char and start_char < len(text):\n",
    "                for i in range(start_char, end_char):\n",
    "                    if i == start_char:\n",
    "                        char_labels[i] = f\"B-{entity_type}\"\n",
    "                    else:\n",
    "                        char_labels[i] = f\"I-{entity_type}\"\n",
    "\n",
    "        # 3. 토큰화 및 토큰-문자 정렬\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length'  # 패딩 추가\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        offset_mapping = tokenized[\"offset_mapping\"]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # 4. 토큰별 레이블 할당 (개선된 BIO 태깅)\n",
    "        ner_token_labels = []\n",
    "        prev_entity_type = None\n",
    "\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            # 특수 토큰 또는 패딩 토큰 처리\n",
    "            if start == end or attention_mask[i] == 0:\n",
    "                token_label = -100  # ignore_index\n",
    "                ner_token_labels.append(token_label)\n",
    "                prev_entity_type = None\n",
    "                continue\n",
    "\n",
    "            # 토큰에 해당하는 문자 레이블 모음\n",
    "            token_char_labels = [char_labels[j] for j in range(start, end)]\n",
    "\n",
    "            # 토큰 내 문자 레이블들 중 B- 태그가 있으면 B- 우선 (한국어 형태소에 적합)\n",
    "            b_labels = [l for l in token_char_labels if l.startswith(\"B-\")]\n",
    "            i_labels = [l for l in token_char_labels if l.startswith(\"I-\")]\n",
    "\n",
    "            if b_labels:\n",
    "                # B- 태그가 여러 개 있으면 첫 번째 것 사용\n",
    "                token_label = ner_label_to_id[b_labels[0]]\n",
    "                prev_entity_type = b_labels[0][2:]  # \"B-\"를 제외한 엔티티 유형\n",
    "            elif i_labels:\n",
    "                # 이전 토큰이 같은 엔티티 유형이었으면 I- 태그 유지\n",
    "                if prev_entity_type and any(l[2:] == prev_entity_type for l in i_labels):\n",
    "                    token_label = ner_label_to_id[f\"I-{prev_entity_type}\"]\n",
    "                else:\n",
    "                    # 그렇지 않으면 새로운 엔티티 시작으로 취급 (B- 태그로 변환)\n",
    "                    entity_type = i_labels[0][2:]  # \"I-\"를 제외한 엔티티 유형\n",
    "                    token_label = ner_label_to_id[f\"B-{entity_type}\"]\n",
    "                    prev_entity_type = entity_type\n",
    "            else:\n",
    "                token_label = ner_label_to_id[\"O\"]\n",
    "                prev_entity_type = None\n",
    "\n",
    "            ner_token_labels.append(token_label)\n",
    "\n",
    "        # 레이블 길이 확인 및 조정\n",
    "        if len(ner_token_labels) < len(input_ids):\n",
    "            ner_token_labels.extend([-100] * (len(input_ids) - len(ner_token_labels)))\n",
    "        elif len(ner_token_labels) > len(input_ids):\n",
    "            ner_token_labels = ner_token_labels[:len(input_ids)]\n",
    "\n",
    "        if idx < 3:  # 샘플 출력\n",
    "            print(f\"\\n[NER 데이터 {idx}] '{text}' -> 엔티티: {entities}\")\n",
    "            print(f\"  인텐트: {intent_id_to_label.get(intent_label, '알 수 없음')}\")\n",
    "            print(f\"  토큰화: {tokens[:10]}... (총 {len(tokens)}개)\")\n",
    "            print(f\"  NER 레이블: {[ner_id_to_label.get(l, 'IGN') for l in ner_token_labels[:10]]}... (총 {len(ner_token_labels)}개)\")\n",
    "\n",
    "        integrated_data.append({\n",
    "            \"text\": text,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"intent_label\": intent_label,\n",
    "            \"ner_labels\": ner_token_labels\n",
    "        })\n",
    "\n",
    "        # 데이터 증강: 엔티티가 많은 데이터는 중복해서 추가\n",
    "        num_entities = len(entities)\n",
    "        if num_entities > 2:  # 엔티티가 3개 이상이면 중복 추가\n",
    "            integrated_data.append({\n",
    "                \"text\": text + \" (duplicated)\",\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"intent_label\": intent_label,\n",
    "                \"ner_labels\": ner_token_labels\n",
    "            })\n",
    "\n",
    "    print(f\"\\n통합 데이터셋 크기: {len(integrated_data)}\")\n",
    "\n",
    "    # 3.6 데이터셋 생성\n",
    "    integrated_features = Features({\n",
    "        'text': Value('string'),\n",
    "        'input_ids': Sequence(Value('int32'), length=MAX_LEN),\n",
    "        'attention_mask': Sequence(Value('int32'), length=MAX_LEN),\n",
    "        'intent_label': Value('int32'),\n",
    "        'ner_labels': Sequence(Value('int32'), length=MAX_LEN)\n",
    "    })\n",
    "\n",
    "    integrated_dataset = Dataset.from_list(integrated_data, features=integrated_features)\n",
    "\n",
    "    # 학습/검증/테스트 데이터셋 분리\n",
    "    train_val_test_datasets = integrated_dataset.train_test_split(test_size=0.3, seed=SEED)\n",
    "    train_dataset = train_val_test_datasets[\"train\"]\n",
    "\n",
    "    # 테스트 데이터를 검증과 테스트로 분리\n",
    "    test_valid_datasets = train_val_test_datasets[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "    valid_dataset = test_valid_datasets[\"train\"]\n",
    "    test_dataset = test_valid_datasets[\"test\"]\n",
    "\n",
    "    train_dataset = limit_dataset_size(train_dataset, max_size=1000)\n",
    "    valid_dataset = limit_dataset_size(valid_dataset, max_size=300)\n",
    "    test_dataset = limit_dataset_size(test_dataset, max_size=300)\n",
    "\n",
    "    print(f\"훈련 데이터 크기: {len(train_dataset)}\")\n",
    "    print(f\"검증 데이터 크기: {len(valid_dataset)}\")\n",
    "    print(f\"테스트 데이터 크기: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, intent_label_to_id, intent_id_to_label, ner_label_to_id, ner_id_to_label\n",
    "\n",
    "# --- 4. 데이터 콜레이터 정의 ---\n",
    "class ImprovedJointNLUDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # 입력 시퀀스 최대 길이 계산\n",
    "        max_length = max(len(feature[\"input_ids\"]) for feature in features)\n",
    "        max_length = min(max_length, MAX_LEN)  # 최대 길이 제한\n",
    "\n",
    "        # 배치 준비\n",
    "        batch = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "            \"intent_labels\": [],\n",
    "            \"ner_labels\": []\n",
    "        }\n",
    "\n",
    "        for feature in features:\n",
    "            # 패딩 적용\n",
    "            input_ids = feature[\"input_ids\"]\n",
    "            attention_mask = feature[\"attention_mask\"]\n",
    "            ner_labels = feature[\"ner_labels\"]\n",
    "\n",
    "            # 길이 확인 및 조정\n",
    "            if len(input_ids) > max_length:\n",
    "                input_ids = input_ids[:max_length]\n",
    "                attention_mask = attention_mask[:max_length]\n",
    "                ner_labels = ner_labels[:max_length]\n",
    "\n",
    "            padding_length = max_length - len(input_ids)\n",
    "\n",
    "            # 입력 패딩\n",
    "            if padding_length > 0:\n",
    "                input_ids = input_ids + [self.pad_token_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "                ner_labels = ner_labels + [-100] * padding_length  # -100은 손실 계산에서 무시됨\n",
    "\n",
    "            # 배치에 추가\n",
    "            batch[\"input_ids\"].append(input_ids)\n",
    "            batch[\"attention_mask\"].append(attention_mask)\n",
    "            batch[\"intent_labels\"].append(feature.get(\"intent_label\", 0))\n",
    "            batch[\"ner_labels\"].append(ner_labels)\n",
    "\n",
    "        # 텐서로 변환\n",
    "        batch[\"input_ids\"] = torch.tensor(batch[\"input_ids\"], dtype=torch.long)\n",
    "        batch[\"attention_mask\"] = torch.tensor(batch[\"attention_mask\"], dtype=torch.long)\n",
    "        batch[\"intent_labels\"] = torch.tensor(batch[\"intent_labels\"], dtype=torch.long)\n",
    "        batch[\"ner_labels\"] = torch.tensor(batch[\"ner_labels\"], dtype=torch.long)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# --- 5. 개선된 커스텀 트레이너 정의 (통합 NLU용) ---\n",
    "class ImprovedJointNLUTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_metrics = {'joint_score': 0}\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # 안정적인 손실 계산을 위한 예외 처리 추가\n",
    "        try:\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                intent_labels=inputs[\"intent_labels\"],\n",
    "                ner_labels=inputs[\"ner_labels\"]\n",
    "            )\n",
    "\n",
    "            loss = outputs[\"loss\"]\n",
    "            intent_loss = outputs[\"intent_loss\"]\n",
    "            ner_loss = outputs[\"ner_loss\"]\n",
    "\n",
    "            # 손실 값 검증 (NaN 방지)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                logger.warning(\"발견된 NaN/Inf 손실값! 대체값 사용...\")\n",
    "                loss = torch.tensor(1.0, device=loss.device, requires_grad=True)\n",
    "\n",
    "            # 자세한 로깅 (정기적으로)\n",
    "            if self.state.global_step % 10 == 0:\n",
    "                self.log({\n",
    "                    \"total_loss\": loss.item(),\n",
    "                    \"intent_loss\": intent_loss.item() if intent_loss is not None else 0,\n",
    "                    \"ner_loss\": ner_loss.item() if ner_loss is not None else 0,\n",
    "                    \"lr\": self.optimizer.param_groups[0]['lr']  # 현재 학습률 로깅\n",
    "                })\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"손실 계산 중 오류 발생: {e}\")\n",
    "            # 오류 시 기본 손실 반환\n",
    "            loss = torch.tensor(1.0, device=inputs[\"input_ids\"].device, requires_grad=True)\n",
    "            return (loss, None) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        with torch.no_grad():\n",
    "            # 입력 데이터 검증\n",
    "            for k, v in inputs.items():\n",
    "                if torch.isnan(v).any() or torch.isinf(v).any():\n",
    "                    logger.warning(f\"입력 데이터 문제 발견 ({k})!\")\n",
    "                    inputs[k] = torch.nan_to_num(v)  # NaN/Inf 값 대체\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                intent_labels=inputs[\"intent_labels\"],\n",
    "                ner_labels=inputs[\"ner_labels\"]\n",
    "            )\n",
    "\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "            if prediction_loss_only:\n",
    "                return (loss, None, None)\n",
    "\n",
    "            # 예측 및 레이블 추출\n",
    "            intent_logits = outputs[\"intent_logits\"]\n",
    "            intent_predictions = torch.argmax(intent_logits, dim=1)\n",
    "            intent_labels = inputs[\"intent_labels\"]\n",
    "\n",
    "            # CRF 기반 NER 예측 처리\n",
    "            ner_logits = outputs[\"ner_logits\"]\n",
    "            attention_mask = inputs[\"attention_mask\"].bool()\n",
    "            ner_predictions = torch.zeros_like(inputs[\"input_ids\"])\n",
    "\n",
    "            try:\n",
    "                # CRF 디코딩 (최적의 태그 시퀀스 찾기)\n",
    "                best_ner_tags_list = model.crf.decode(ner_logits, mask=attention_mask)\n",
    "\n",
    "                for i, tags in enumerate(best_ner_tags_list):\n",
    "                    length = min(len(tags), ner_predictions.size(1))\n",
    "                    ner_predictions[i, :length] = torch.tensor(tags[:length], device=ner_predictions.device)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"NER 예측 중 오류 발생: {e}\")\n",
    "                # 오류 시 모든 토큰을 'O'(Outside) 태그로 예측\n",
    "                ner_predictions = torch.zeros_like(inputs[\"input_ids\"])\n",
    "\n",
    "            ner_labels = inputs[\"ner_labels\"]\n",
    "\n",
    "            # 반환값: (손실, 예측값 튜플, 레이블 튜플)\n",
    "            return (loss, (intent_predictions, ner_predictions), (intent_labels, ner_labels))\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"향상된 평가 메서드: 점진적 평가 및 상세 메트릭스\"\"\"\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "\n",
    "        # 개선된 결과 저장 로직 (새로운 최고 성능 달성 시)\n",
    "        current_score = metrics.get(f\"{metric_key_prefix}_joint_score\", 0)\n",
    "        if current_score > self.best_metrics['joint_score']:\n",
    "            self.best_metrics['joint_score'] = current_score\n",
    "            self.best_metrics['step'] = self.state.global_step\n",
    "            self.best_metrics['epoch'] = self.state.epoch\n",
    "\n",
    "            # 상세 로깅\n",
    "            logger.info(f\"새로운 최고 성능 달성! Joint Score: {current_score:.4f} (Step: {self.state.global_step}, Epoch: {self.state.epoch:.2f})\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "# --- 6. 개선된 평가 지표 계산 함수 ---\n",
    "def compute_improved_joint_metrics(eval_preds, ner_label_list=None):\n",
    "    (intent_preds, ner_preds), (intent_labels, ner_labels) = eval_preds\n",
    "\n",
    "    # Intent 평가 (F1 스코어 추가)\n",
    "    intent_accuracy = accuracy_score(intent_labels, intent_preds)\n",
    "    intent_f1_macro = sklearn_f1_score(intent_labels, intent_preds, average='macro')\n",
    "    intent_f1_weighted = sklearn_f1_score(intent_labels, intent_preds, average='weighted')\n",
    "\n",
    "    # NER 평가 준비\n",
    "    true_ner_predictions = []\n",
    "    true_ner_labels = []\n",
    "\n",
    "    for prediction, label, mask in zip(ner_preds, ner_labels, attention_mask if 'attention_mask' in eval_preds else None):\n",
    "        true_pred = []\n",
    "        true_label = []\n",
    "\n",
    "        for p, l in zip(prediction, label):\n",
    "            if l != -100:  # -100은 무시\n",
    "                # NER 레이블 ID를 문자열 레이블로 변환\n",
    "                try:\n",
    "                    true_pred.append(ner_id_to_label[p.item()])\n",
    "                    true_label.append(ner_id_to_label[l.item()])\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"레이블 변환 오류: {e} - p={p.item()}, l={l.item()}\")\n",
    "                    true_pred.append(\"O\")  # 오류 시 기본값\n",
    "                    true_label.append(\"O\")\n",
    "\n",
    "        true_ner_predictions.append(true_pred)\n",
    "        true_ner_labels.append(true_label)\n",
    "\n",
    "    # seqeval의 f1_score 계산 (더 상세한 메트릭스)\n",
    "    try:\n",
    "        ner_f1 = f1_score(true_ner_labels, true_ner_predictions)\n",
    "        ner_report = classification_report(true_ner_labels, true_ner_predictions, digits=4, output_dict=True)\n",
    "\n",
    "        # 개체 유형별 F1 점수 추출\n",
    "        entity_metrics = {}\n",
    "        for label, metrics in ner_report.items():\n",
    "            if label not in ['micro avg', 'macro avg', 'weighted avg'] and isinstance(metrics, dict):\n",
    "                entity_metrics[f\"ner_f1_{label}\"] = metrics.get('f1-score', 0)\n",
    "\n",
    "        # 평가 보고서 출력 (텍스트 형식)\n",
    "        print(\"\\nNER Classification Report:\\n\", classification_report(true_ner_labels, true_ner_predictions, digits=4))\n",
    "\n",
    "        # 통합 점수 계산 (가중 평균, NER에 더 높은 가중치)\n",
    "        joint_score = INTENT_WEIGHT * intent_accuracy + NER_WEIGHT * ner_f1\n",
    "\n",
    "        # 최종 메트릭스 반환 (상세한 지표 포함)\n",
    "        metrics = {\n",
    "            \"intent_accuracy\": intent_accuracy,\n",
    "            \"intent_f1_macro\": intent_f1_macro,\n",
    "            \"intent_f1_weighted\": intent_f1_weighted,\n",
    "            \"ner_f1\": ner_f1,\n",
    "            \"joint_score\": joint_score,\n",
    "            **entity_metrics  # 개체 유형별 성능 추가\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"평가 지표 계산 오류: {e}\")\n",
    "        return {\n",
    "            \"intent_accuracy\": intent_accuracy,\n",
    "            \"intent_f1_macro\": intent_f1_macro,\n",
    "            \"intent_f1_weighted\": intent_f1_weighted,\n",
    "            \"ner_f1\": 0.0,\n",
    "            \"joint_score\": intent_accuracy * INTENT_WEIGHT\n",
    "        }\n",
    "\n",
    "# --- 7. 학습률 스케줄러 개선 ---\n",
    "def get_linear_schedule_with_warmup_and_decay(optimizer, num_warmup_steps, num_training_steps, min_lr_ratio=0.1):\n",
    "    \"\"\"학습률 워밍업 및 선형 감소 스케줄러\"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 워밍업 구간\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "\n",
    "        # 워밍업 이후 선형 감소\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(min_lr_ratio, 1.0 - progress)\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# --- 8. 개선된 통합 NLU 모델 훈련 함수 ---\n",
    "def train_improved_integrated_nlu_model():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"개선된 통합 NLU 모델 훈련 시작 (Intent + NER)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 글로벌 변수 설정 (평가 지표용)\n",
    "    global intent_id_to_label, ner_id_to_label, tokenizer\n",
    "\n",
    "    # 8.1 데이터 전처리 및 통합\n",
    "    train_dataset, valid_dataset, test_dataset, intent_label_to_id, intent_id_to_label, ner_label_to_id, ner_id_to_label = preprocess_and_merge_data()\n",
    "\n",
    "    # 8.2 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # 8.3 개선된 데이터 콜레이터 초기화\n",
    "    data_collator = ImprovedJointNLUDataCollator(tokenizer)\n",
    "\n",
    "    # 8.4 모델 설정\n",
    "    config = AutoModel.from_pretrained(MODEL_NAME).config\n",
    "    config.intent_label_to_id = intent_label_to_id\n",
    "    config.ner_label_to_id = ner_label_to_id\n",
    "\n",
    "    # 8.5 개선된 통합 NLU 모델 초기화\n",
    "    model = ImprovedRobertaForJointIntentAndNER(\n",
    "        config,\n",
    "        intent_label_to_id,\n",
    "        ner_label_to_id\n",
    "    )\n",
    "\n",
    "    # 8.6 개선된 훈련 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results/improved_integrated_nlu\",\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=1,  # 그래디언트 누적 단순화\n",
    "        logging_dir='./logs/improved_integrated_nlu',\n",
    "        logging_steps=10,\n",
    "        save_steps=100,  # 저장 빈도 감소\n",
    "        eval_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=1,  # 최고 성능 모델 더 많이 저장\n",
    "        fp16=True,  # 혼합 정밀도 훈련 활성화 (속도 향상)\n",
    "        greater_is_better=True,\n",
    "        metric_for_best_model=\"intent_accuracy\",\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        lr_scheduler_type=\"linear\",  # 개선된 스케줄러\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # 8.7 개선된 훈련 시작\n",
    "    trainer = ImprovedJointNLUTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_improved_joint_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # 메모리 최적화\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    # 훈련 실행\n",
    "    print(\"모델 훈련 시작...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 8.8 최종 평가\n",
    "    print(\"\\n--- 검증 세트 최종 평가 ---\")\n",
    "    eval_result = trainer.evaluate(valid_dataset)\n",
    "    print(f\"검증 세트 평가 결과: {eval_result}\")\n",
    "\n",
    "    print(\"\\n--- 테스트 세트 최종 평가 ---\")\n",
    "    test_result = trainer.evaluate(test_dataset, metric_key_prefix=\"test\")\n",
    "    print(f\"테스트 세트 평가 결과: {test_result}\")\n",
    "\n",
    "    # 8.9 모델 및 레이블 정보 저장\n",
    "    print(\"\\n모델 및 레이블 정보 저장 중...\")\n",
    "    os.makedirs(INTEGRATED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    # 메모리 최적화\n",
    "    gc.collect()\n",
    "    model.cpu()\n",
    "\n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), os.path.join(INTEGRATED_MODEL_DIR, \"pytorch_model.bin\"))\n",
    "\n",
    "    # Config 정보 저장\n",
    "    config_dict = config.to_dict()\n",
    "    config_dict[\"model_type\"] = \"improved_roberta_joint_nlu\"  # 개선된 모델 타입 표시\n",
    "    with open(os.path.join(INTEGRATED_MODEL_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 토크나이저 저장\n",
    "    tokenizer.save_pretrained(INTEGRATED_MODEL_DIR)\n",
    "\n",
    "    # 레이블 매핑 저장\n",
    "    with open(INTEGRATED_LABEL_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"intent_id2label\": intent_id_to_label,\n",
    "            \"intent_label2id\": intent_label_to_id,\n",
    "            \"ner_id2label\": ner_id_to_label,\n",
    "            \"ner_label2id\": ner_label_to_id,\n",
    "            \"training_metrics\": {\n",
    "                \"validation\": eval_result,\n",
    "                \"test\": test_result,\n",
    "                \"best_metrics\": trainer.best_metrics\n",
    "            }\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"개선된 통합 NLU 모델 및 레이블 정보 저장 완료: {INTEGRATED_MODEL_DIR}\")\n",
    "    print(\"개선된 통합 NLU 모델 훈련 완료!\")\n",
    "\n",
    "# --- 9. 메인 실행 코드 ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 시작 시간 기록\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 개선된 훈련 함수 실행\n",
    "        train_improved_integrated_nlu_model()\n",
    "\n",
    "        # 총 실행 시간 출력\n",
    "        total_time = time.time() - start_time\n",
    "        hours, remainder = divmod(total_time, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        print(f\"\\n총 실행 시간: {int(hours)}시간 {int(minutes)}분 {seconds:.2f}초\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"모델 훈련 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
